{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "SPFlow is an open-source functional-oriented Python package for Probabilistic Circuits (PCs) with ready-to-use implementations for Sum-Product Networks (SPNs). PCs are a class of powerful deep probabilistic models - expressible as directed acyclic graphs - that allow for tractable querying. This library provides routines for creating, learning, manipulating and interacting with PCs and is highly extensible and customizable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Creating Models:\n",
    "\n",
    "In this section we will show how to create simple Sum-Product networks based on sum and product layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Scopes\n",
    "\n",
    "Scope objects represent scopes of features, denoted by indices. A scope contains two parts. \n",
    "The query of the scopes indicate the features, the scope represents. \n",
    "The evidence of the scopes contains any features that the query features are conditionals of (empty for non-conditional scopes).\n",
    "Scopes are generally only explicitly specified for leaf distributions, and inferred recursively for nodes higher in the graph structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from spflow.meta.data import Scope\n",
    "\n",
    "scope = Scope([0,4,3], [1,2]) # scope over indices 0,4,3 conditional on indices 1,2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Modules\n",
    "\n",
    "Models in SPFlow are built from modules, that can represent one or more nodes in the form of a layer.\n",
    "A layer is defined by its event shape (out_features, out_channel, num_repetitions). The number of output features and\n",
    "the number of output channel are mandatory. The number of output features is implicitly derived from the scope of the module,\n",
    "whereas the number of output channel either has to be passed explicitly or can be derived from the module parameters.\n",
    "The number of repetitions is optional, and therefore it can be None.\n",
    "In general, each module expects one input Module. Exceptions will be discussed later in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Leaf Layer\n",
    "\n",
    "The leaf layer represents the lowest layer in the SPN and can be created for different distributions.\n",
    "The event shape can be derived either from given parameters or by explicitly passing the shape as intitialization parameter.\n",
    "For example a leaf layer with a Normal distribution can be initialized via\n",
    "\n",
    "- init parameters: Then a random mean and variance is generated or,\n",
    "- distribution parameter shape: If mean and variance is given, the event shape can be derived from the shape of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from spflow.modules.leaf import Normal\n",
    "\n",
    "out_feature = 2\n",
    "out_channels = 4\n",
    "scope_normal = Scope([0,1])\n",
    "\n",
    "mean = torch.randn((out_feature,out_channels))\n",
    "std = torch.rand((out_feature,out_channels))\n",
    "\n",
    "normal_layer = Normal(scope=scope_normal, mean=mean, std=std) # init via parameters\n",
    "\n",
    "normal_layer2 = Normal(scope=scope_normal, out_channels=out_channels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following distributions are implemented as leaf module: Bernoulli, Binomial, Categorical, Exponential, Gamma,\n",
    "Geometric, Hypergeometric, LogNormal, NegativeBinomial, Normal, Poisson and Uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Sum layer\n",
    "\n",
    "The sum layer calculates the sum over the weighted output of its input module.\n",
    "Similar to the leaf layer, the sum layer can be initialized by explicitly passing the number of out_channel or\n",
    "by passing the weight matrix. From the weight matrix, the module can derive the event shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from spflow.modules import Sum\n",
    "\n",
    "sum_out_channels = 5\n",
    "weights = torch.rand((out_feature, out_channels, sum_out_channels))\n",
    "weights /= torch.sum(weights, axis=1, keepdims=True)\n",
    "\n",
    "sum_layer = Sum(inputs=normal_layer, weights=weights)\n",
    "\n",
    "sum_layer2 = Sum(inputs=normal_layer, out_channels=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Product layer\n",
    "\n",
    "The product layer calculates the product over all input features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from spflow.modules import Product\n",
    "\n",
    "product_layer = Product(inputs=normal_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we can create a model by stacking sum and product layers at will with a leaf layer as lowest layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "out_channels = 4\n",
    "scope_normal = Scope([0,1])\n",
    "\n",
    "normal_layer = Normal(scope=scope_normal, out_channels=out_channels)\n",
    "product_layer = Product(inputs=normal_layer)\n",
    "spn = Sum(inputs=product_layer, out_channels=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Inference\n",
    "\n",
    "For inference, SPFlow offers the computation of the log-likelihoods of data for models. Inference takes a two-dimensional data set and returns a two-dimensional array containing the (log-)likelihoods, where each row corresponds to an input data instance.\n",
    "The number of columns corresponds to the number of outputs of the module inference is performed on.\n",
    "Missing data (i.e., NaN values) is implicitly marginalized over. This means that a completely missing data instance (all NaN row) outputs a likelihood of 1 (and corresponding log-likelihood of 0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from spflow import log_likelihood\n",
    "\n",
    "num_instances = 10\n",
    "num_feature = 2\n",
    "\n",
    "data = torch.randn((num_instances,num_feature))\n",
    "ll = log_likelihood(spn, data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Sampling\n",
    "\n",
    "SPFlow can also sample data from models, possibly in the presence of evidence.\n",
    "Generating a sample from a model can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from spflow import sample\n",
    "\n",
    "samples = sample(spn)\n",
    "\n",
    "# Drawing multiple samples at once can be done similary, by providing the number of target samples to generate:\n",
    "samples = sample(spn, 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Sampling with evidence:\n",
    "In the case of evidence, a partially filled data set is passed to sample instead.\n",
    "The routine fills the data tensor in-place, taking specified evidence into account.\n",
    "Keeping track of the return value should not be necessary in this case, since the input data set should be modified in-place.\n",
    "However, it is good-practice to (re-)assign it nonetheless.\n",
    "Note, that sample(model) and sample(model, n) are simply convenient aliases that create an empty data set of appropriate shape to fill with generated values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# one feature has given values\n",
    "evidence = torch.randn((num_instances,1))\n",
    "\n",
    "nan_tensor = torch.full((num_instances,1), fill_value=torch.nan)\n",
    "data = torch.concatenate((nan_tensor,evidence), dim=1)\n",
    "\n",
    "data = sample(spn, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Training models:\n",
    "Both EM and a Gradient Descent are already implemented within this library.\n",
    "\n",
    "Expectation Maximization:\n",
    "Each module has an EM method which allows applying EM on any architecture.\n",
    "The necessary mle methods are also already implemented in each of the given distributions.\n",
    "\n",
    "Gradient descent:\n",
    "To train your model with gradient descent, there is a template method train_gradient_descent, in which all necessary hyperparameters can be selected as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-9.8831, -2.7096, -2.7096])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spflow.learn import expectation_maximization\n",
    "\n",
    "num_instances = 10\n",
    "num_feature = 2\n",
    "out_channels = 4\n",
    "scope_normal = Scope([0,1])\n",
    "\n",
    "normal_layer = Normal(scope=scope_normal, out_channels=out_channels)\n",
    "product_layer = Product(inputs=normal_layer)\n",
    "spn = Sum(inputs=product_layer, out_channels=1)\n",
    "\n",
    "data = torch.randn((num_instances,num_feature))\n",
    "expectation_maximization(module=spn, data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Gradient descent:\n",
    "To train your model with gradient descent, there is a template method train_gradient_descent, in which all necessary hyperparameters can be selected as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\pycharm\\pycharmprojects\\spflowtorch\\venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from spflow.learn import train_gradient_descent\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "num_instances = 10\n",
    "num_feature = 2\n",
    "out_channels = 4\n",
    "scope_normal = Scope([0,1])\n",
    "\n",
    "normal_layer = Normal(scope=scope_normal, out_channels=out_channels)\n",
    "product_layer = Product(inputs=normal_layer)\n",
    "spn = Sum(inputs=product_layer, out_channels=1)\n",
    "\n",
    "data = torch.randn((num_instances,num_feature))\n",
    "dataset = TensorDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=10)\n",
    "\n",
    "train_gradient_descent(spn, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Advanced layers\n",
    "\n",
    "In addition to the basic sum and product layers, there are also more advanced layers that are implemented in this library:\n",
    "Elementwise-Sum layer, Outer-product layer, Elementwise-Product layer.\n",
    "\n",
    "All layers above expect either a list of modules or a Split module as input.\n",
    "\n",
    "A Split module is a module that takes a single module as input and splits it for further usage.\n",
    "In general, in a Split module you can define the number of splits and the dimension you want to split along (dim 1 = feature dim, dim 2 = channel dim)\n",
    "There are two specific types of Split modules that are already implemented:\n",
    "The first one is the SplitHalves module.\n",
    "It splits the module in the middle e.g. [0,1,2,3,4,5] -> [0,1,2],[3,4,5].\n",
    "The other Split module is the SplitAlternate module.\n",
    "It splits the module in an alternating fashion e.g. [0,1,2,3,4,5] -> [0,2,4],[1,3,5]\n",
    "\n",
    "A Split module can be created like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2], [3, 4, 5]]\n",
      "[[0, 2, 4], [1, 3, 5]]\n"
     ]
    }
   ],
   "source": [
    "from spflow.modules.ops.split_halves import SplitHalves\n",
    "from spflow.modules.ops.split_alternate import SplitAlternate\n",
    "\n",
    "scope_normal = Scope([0,1,2,3,4,5])\n",
    "out_channels = 4\n",
    "\n",
    "normal_layer = Normal(scope=scope_normal, out_channels=out_channels)\n",
    "\n",
    "split_halves = SplitHalves(inputs=normal_layer, dim=1, num_splits=2)\n",
    "print(split_halves.feature_to_scope)\n",
    "\n",
    "split_alternate = SplitAlternate(inputs=normal_layer, dim=1, num_splits=2)\n",
    "print(split_alternate.feature_to_scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Elementwise Sum\n",
    "\n",
    "The elementwise sum module calculates the elementwise sum over the output channel of the input modules.\n",
    "Therefore, the input modules need to have the same number of output channel or number of output channel must be broadcastable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from spflow.modules.elementwise_sum import ElementwiseSum\n",
    "\n",
    "scope_normal = Scope([0,1,2,3,4,5])\n",
    "leaf_out_channels = 4\n",
    "sum_out_channels = 2\n",
    "\n",
    "normal_layer_1 = Normal(scope=scope_normal, out_channels=leaf_out_channels)\n",
    "normal_layer_2 = Normal(scope=scope_normal, out_channels=leaf_out_channels)\n",
    "\n",
    "elementwise_sum = ElementwiseSum(inputs=[normal_layer_1,normal_layer_2], out_channels=sum_out_channels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Elementwise Product\n",
    "\n",
    "In contrast to the default product layer, where the module multiplies the input along the feature dimension,\n",
    "the elementwise product layer calculates the elementwise product over the output channel.\n",
    "The input either has to be a list of inputs with disjoint scopes or a split module.\n",
    "\n",
    "Example:\n",
    "input 1 with 3 out_channel(OC) [OC_1_0,OC_1_1,OC_1_2]\n",
    "input 2 with 3 out_channel(OC) [OC_2_0,OC_2_1,OC_2_2]\n",
    "result: [OC_1_0 * OC_2_0,OC_1_1 * OC_2_1,OC_1_2 * OC_2_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_features:  3\n",
      "num_channel:  3\n"
     ]
    }
   ],
   "source": [
    "from spflow.modules.elementwise_product import ElementwiseProduct\n",
    "scope_normal_1 = Scope([0,1,2])\n",
    "scope_normal_2 = Scope([3,4,5])\n",
    "leaf_out_channels = 3\n",
    "\n",
    "normal_layer_1 = Normal(scope=scope_normal_1, out_channels=leaf_out_channels)\n",
    "normal_layer_2 = Normal(scope=scope_normal_2, out_channels=leaf_out_channels)\n",
    "\n",
    "elementwise_prod = ElementwiseProduct(inputs=[normal_layer_1, normal_layer_2])\n",
    "print(\"num_features: \", elementwise_prod.out_features)\n",
    "print(\"num_channel: \", elementwise_prod.out_channels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Outer Product\n",
    "\n",
    "This product layer calculates the outer product over the output channel.\n",
    "Again, the input either has to be a list of inputs with disjoint scopes or a split module.\n",
    "\n",
    "Example:\n",
    "input 1 with 3 out_channel(OC) [OC_1_0,OC_1_1,OC_1_2]\n",
    "input 2 with 3 out_channel(OC) [OC_2_0,OC_2_1,OC_2_2]\n",
    "result: [OC_1_0 * OC_2_0, OC_1_0 * OC_2_1, OC_1_0 * OC_2_2]\n",
    "        [OC_1_1 * OC_2_0, OC_1_1 * OC_2_1, OC_1_1 * OC_2_2]\n",
    "        [OC_1_2 * OC_2_0, OC_1_2 * OC_2_1, OC_1_2 * OC_2_2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_features:  3\n",
      "num_channel:  9\n"
     ]
    }
   ],
   "source": [
    "from spflow.modules.outer_product import OuterProduct\n",
    "scope_normal_1 = Scope([0,1,2])\n",
    "scope_normal_2 = Scope([3,4,5])\n",
    "leaf_out_channels = 3\n",
    "\n",
    "normal_layer_1 = Normal(scope=scope_normal_1, out_channels=leaf_out_channels)\n",
    "normal_layer_2 = Normal(scope=scope_normal_2, out_channels=leaf_out_channels)\n",
    "\n",
    "elementwise_prod = OuterProduct(inputs=[normal_layer_1, normal_layer_2])\n",
    "print(\"num_features: \", elementwise_prod.out_features)\n",
    "print(\"num_channel: \", elementwise_prod.out_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Sampling Context\n",
    "\n",
    "The SamplingContext class controls the sampling process and is passed to the sampling routing. It is mostly used internally, although users can use it manually if needed.\n",
    "It consists of 3 parts:\n",
    "1. channel index: The channel index is a tensor that indicates the channel from which the corresponding feature is supposed to be sampled for each instance.\n",
    "Therefore it has the form (batch_size, out_features) with values between [0, out_channel].\n",
    "2. mask: The mask is applied on the samples and indicates which samples should be considered. It has the same shape as the channel index\n",
    "3. repetition index: If the model has a repetition dimension the repetition index indicates from which repetition each instance is supposed to be sampled.\n",
    "If the model has no repetition dimension the index is None.\n",
    "\n",
    "In the example below, we define that for the first instance feature 0 gets sampled from channel 0, feature 1 from channel 1 etc.\n",
    "For the second instance, we use channel 0 for all features.\n",
    "Additionally we define via the repetition index that we sample the first instance from repetition 0 and the second instance from repetition 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from spflow.meta.dispatch import SamplingContext\n",
    "\n",
    "scope = Scope([0,1,2,3,4,5])\n",
    "module = Normal(scope=scope, out_channels=6, num_repetitions=3)\n",
    "\n",
    "# Setup sampling context\n",
    "n_samples = 2\n",
    "data = torch.full((n_samples, 6), torch.nan)\n",
    "channel_index = torch.tensor([[0,1,2,3,4,5],[0,0,0,0,0,0]], dtype=torch.int64)\n",
    "mask = torch.full((n_samples, 6), True, dtype=torch.bool)\n",
    "\n",
    "repetition_index = torch.tensor([0,1], dtype=torch.int64)\n",
    "\n",
    "sampling_ctx = SamplingContext(channel_index=channel_index, mask=mask, repetition_index=repetition_index)\n",
    "\n",
    "# Sample\n",
    "samples = sample(module, data, sampling_ctx=sampling_ctx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Dispatch:\n",
    "\n",
    "Internally, SPFlow uses dispatch to call the correct implementation based on the specified module classes.\n",
    "All dispatched functions accept a dispatch_ctx keyword argument, taking a DispatchContext instance.\n",
    "Amongst other things, this takes care of memoization. In most cases, the dispatch context is created automatically. However, there are a few scenarios in which users might want to deal with the dispatch context directly. For example, for routines using memoization, like (log-)likelihood, the dispatch context stores the outputs of all modules:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Normal(\n",
      "  D=2, C=4, R=None\n",
      "  (distribution): Normal()\n",
      "): tensor([[[ -28.2992,   -1.3575,   -1.1106,  -11.2995],\n",
      "         [  -5.0287,   -3.8192,   -2.4297,  -30.9220]],\n",
      "\n",
      "        [[-118.7968,    0.9387,   -0.8218,   -8.4038],\n",
      "         [  -2.6555,  -79.7616,  -22.5136,   -2.4236]]],\n",
      "       grad_fn=<IndexPutBackward0>), Product(\n",
      "  D=1, C=4, R=None\n",
      "  (inputs): Normal(\n",
      "    D=2, C=4, R=None\n",
      "    (distribution): Normal()\n",
      "  )\n",
      "): tensor([[[ -33.3279,   -5.1767,   -3.5402,  -42.2216]],\n",
      "\n",
      "        [[-121.4522,  -78.8230,  -23.3354,  -10.8274]]],\n",
      "       grad_fn=<SumBackward1>), Sum(\n",
      "  D=1, C=1, R=None, weights=(1, 4, 1)\n",
      "  (inputs): Product(\n",
      "    D=1, C=4, R=None\n",
      "    (inputs): Normal(\n",
      "      D=2, C=4, R=None\n",
      "      (distribution): Normal()\n",
      "    )\n",
      "  )\n",
      "): tensor([[[ -3.9496]],\n",
      "\n",
      "        [[-12.3731]]], grad_fn=<ViewBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "from spflow.meta.dispatch import DispatchContext\n",
    "from spflow import log_likelihood\n",
    "\n",
    "# create dispatch context\n",
    "dispatch_ctx = DispatchContext()\n",
    "\n",
    "# compute log likelihoods\n",
    "log_likelihoods = log_likelihood(spn, data, dispatch_ctx=dispatch_ctx)\n",
    "\n",
    "# inspect cached log-likelihood outputs\n",
    "print(dispatch_ctx.cache['log_likelihood'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Implemented structures:\n",
    "\n",
    "Rat-Spn:\n",
    "\n",
    "The rat-spn is a model structure based on ... .  It builds a deep network structure by recursively partitioning the features (variables) into random subsets and alternating between sum and product layers.\n",
    "The following hyperparameters define the structure of the model:\n",
    "- leaf_modules: A list of Leaf modules that define the base distributions and their corresponding scope,\n",
    "- n_root_nodes: The number of classes,\n",
    "- n_region_nodes: The number of sums / number of output channel for each sum layer,\n",
    "- num_repetitions: The number of repetitions,\n",
    "- depth: Depth of the network,\n",
    "- outer_product: True if you want to use the outer product Layer and False for the elementwise product layer ,\n",
    "- split_halves: True if you want to use the Split halves split module and False for the alternating split module\n",
    "- num_splits: The number of splits for each split module, default=2,\n",
    "\n",
    "The Rat SPN module can be used just like any other module.\n",
    "For classification, the module already has an implementation for the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from spflow.modules.rat.rat_spn import RatSPN\n",
    "\n",
    "depth = 3\n",
    "n_region_nodes = 10\n",
    "num_leaves = 10\n",
    "num_repetitions = 5\n",
    "n_root_nodes = 1\n",
    "num_feature = 128\n",
    "\n",
    "scope = Scope(list(range(0, num_feature)))\n",
    "\n",
    "leaf_layer = Normal(scope=scope, out_channels=5)\n",
    "\n",
    "model = RatSPN(\n",
    "    leaf_modules=[leaf_layer],\n",
    "    n_root_nodes=n_root_nodes,\n",
    "    n_region_nodes=n_region_nodes,\n",
    "    num_repetitions=num_repetitions,\n",
    "    depth=depth,\n",
    "    outer_product=True,\n",
    "    split_halves=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It is also possible to use different base distributions for different scope sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from spflow.modules.leaf import Binomial\n",
    "\n",
    "depth = 3\n",
    "n_region_nodes = 10\n",
    "num_leaves = 10\n",
    "num_repetitions = 5\n",
    "n_root_nodes = 1\n",
    "num_feature = 128\n",
    "\n",
    "scope_1 = Scope(list(range(0, num_feature//2)))\n",
    "scope_2 = Scope(list(range(num_feature//2, num_feature)))\n",
    "\n",
    "leaf_layer_1 = Normal(scope=scope_1, out_channels=5)\n",
    "leaf_layer_2 = Binomial(scope=scope_2, out_channels=5, n=torch.tensor(2))\n",
    "\n",
    "model = RatSPN(\n",
    "    leaf_modules=[leaf_layer_1,leaf_layer_2],\n",
    "    n_root_nodes=n_root_nodes,\n",
    "    n_region_nodes=n_region_nodes,\n",
    "    num_repetitions=num_repetitions,\n",
    "    depth=depth,\n",
    "    outer_product=True,\n",
    "    split_halves=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Another structure that is already implemented is the LearnSPN, based on ... .\n",
    "In contrast to the Rat-Spn, this structure learns the SPN architecture from given data.\n",
    "The following hyperparameters define the structure of the model:\n",
    "- leaf_modules: A list of Leaf modules that define the base distributions and their corresponding scope\n",
    "- out_channels: The number of output channels of each sum layer,\n",
    "- min_features_slice: The number of features from which splitting is no longer performed,\n",
    "- min_instances_slice: The number of instances from which clustering is no longer performed,\n",
    "- clustering_method: The clustering method, default= kmeans,\n",
    "- partitioning_method: The partitioning method, default = rdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used 6 iterations (0.0152s) to cluster 1000 items into 2 clusters\n",
      "used 5 iterations (0.0s) to cluster 497 items into 2 clusters\n",
      "used 2 iterations (0.0s) to cluster 276 items into 2 clusters\n",
      "used 6 iterations (0.0041s) to cluster 101 items into 2 clusters\n",
      "used 3 iterations (0.0s) to cluster 175 items into 2 clusters\n",
      "used 6 iterations (0.0096s) to cluster 101 items into 2 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Pycharm\\PycharmProjects\\SPFlowTorch\\spflow\\learn\\learn_spn.py:366: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3729.)\n",
      "  weights = torch.tensor(w).T.unsqueeze(0).unsqueeze(-1)  # shape(1, num_clusters, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used 8 iterations (0.0065s) to cluster 221 items into 2 clusters\n",
      "used 8 iterations (0.0065s) to cluster 128 items into 2 clusters\n",
      "used 7 iterations (0.0046s) to cluster 105 items into 2 clusters\n",
      "used 4 iterations (0.01s) to cluster 93 items into 2 clusters\n",
      "used 6 iterations (0.0015s) to cluster 503 items into 2 clusters\n",
      "used 6 iterations (0.0067s) to cluster 208 items into 2 clusters\n",
      "used 6 iterations (0.0071s) to cluster 97 items into 2 clusters\n",
      "used 4 iterations (0.0s) to cluster 111 items into 2 clusters\n",
      "used 5 iterations (0.0s) to cluster 295 items into 2 clusters\n",
      "used 5 iterations (0.009s) to cluster 184 items into 2 clusters\n",
      "used 4 iterations (0.0s) to cluster 75 items into 2 clusters\n",
      "used 2 iterations (0.0046s) to cluster 109 items into 2 clusters\n",
      "used 6 iterations (0.0s) to cluster 111 items into 2 clusters\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from spflow.learn.learn_spn import learn_spn\n",
    "\n",
    "# example on the make_moons dataset\n",
    "torch.manual_seed(0)\n",
    "X, y = make_moons(n_samples=1000, noise=0.1, random_state=42)\n",
    "\n",
    "scope = Scope(list(range(2)))\n",
    "normal_layer = Normal(scope=scope, out_channels=4)\n",
    "\n",
    "spn = learn_spn(\n",
    "    torch.tensor(X, dtype=torch.float32),\n",
    "    leaf_modules=normal_layer,\n",
    "    out_channels=1,\n",
    "    min_instances_slice=70,\n",
    "    min_features_slice = 2\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}