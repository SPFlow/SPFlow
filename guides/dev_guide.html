<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html"><link rel="search" title="Search" href="../search.html"><link rel="next" title="API Documentation" href="../api/index.html"><link rel="prev" title="User Guide" href="user_guide.html">

    <!-- Generated with Sphinx 8.2.3 and Furo 2025.09.25 -->
        <title>Developer Guide - SPFlow 1.0.0</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=d111a655" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=580074bf" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/nbsphinx-code-cells.css?v=2aa19091" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=8dab3a3b" />
    
    


<style>
  body {
    --color-code-background: #f2f2f2;
  --color-code-foreground: #1e1e1e;
  --color-brand-primary: #0066cc;
  --color-brand-content: #0066cc;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --color-brand-primary: #4da6ff;
  --color-brand-content: #4da6ff;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --color-brand-primary: #4da6ff;
  --color-brand-content: #4da6ff;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle site navigation sidebar">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc" aria-label="Toggle table of contents sidebar">
<label class="overlay sidebar-overlay" for="__navigation"></label>
<label class="overlay toc-overlay" for="__toc"></label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <span class="icon"><svg><use href="#svg-menu"></use></svg></span>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">SPFlow 1.0.0</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <span class="sidebar-brand-text">SPFlow 1.0.0</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_guide.html">User Guide</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">Developer Guide</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/index.html">API Documentation</a><input aria-label="Toggle navigation of API Documentation" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/base_modules.html">Base Classes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/module_shape.html">Module Shape</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/sums.html">Sum Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/products.html">Product Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/conv.html">Convolutional Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/leaves.html">Leaf Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/operations.html">Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/rat_spn.html">RAT-SPN Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/learning.html">Learning and Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/scope.html">Scope Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/utilities.html">Utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/wrappers.html">Wrapper Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/interfaces.html">Interfaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/exceptions.html">Exceptions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="../_sources/guides/dev_guide.ipynb.txt" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div>
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="Developer-Guide">
<h1>Developer Guide<a class="headerlink" href="#Developer-Guide" title="Link to this heading">¶</a></h1>
<p>This guide describes how to extend the SpFlow library, including adding custom leaf modules with new distributions, implementing variations of sum or product modules, and developing alternative utility modules such as new split modules.</p>
<section id="Leaf-Modules">
<h2>Leaf Modules<a class="headerlink" href="#Leaf-Modules" title="Link to this heading">¶</a></h2>
<p>In this section, we demonstrate how to implement a new leaf module with a custom distribution. As an example, we use a simple toy distribution defined over the range <code class="docutils literal notranslate"><span class="pre">[-10,</span> <span class="pre">10]</span></code>. It includes one trainable parameter, <strong>border</strong>, which divides the domain into “likely’’ and “unlikely’’ regions:</p>
<ul class="simple">
<li><p>values in the interval <code class="docutils literal notranslate"><span class="pre">[border,</span> <span class="pre">10]</span></code> have probability <strong>0.7</strong>,</p></li>
<li><p>values in <code class="docutils literal notranslate"><span class="pre">[-10,</span> <span class="pre">border)</span></code> have probability <strong>0.3</strong>.</p></li>
</ul>
<p>Although this is not a probabilistically valid distribution (e.g., its probabilities do not integrate to 1), it is sufficient to illustrate how to construct a custom leaf module.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">CustomLeaf</span></code> class inherits from <code class="docutils literal notranslate"><span class="pre">LeafModule</span></code>, the base class for all leaf modules. Consequently, all initialization parameters required by <code class="docutils literal notranslate"><span class="pre">LeafModule</span></code> must also be included in the subclass constructor, in addition to any distribution-specific parameters. For a normal distribution, these parameters might be <code class="docutils literal notranslate"><span class="pre">loc</span></code> and <code class="docutils literal notranslate"><span class="pre">scale</span></code>; in our custom example, the required parameter is the trainable <strong>border</strong>. Once included, this parameter can be initialized just like any other PyTorch
parameter.</p>
<p>The only methods that must be implemented manually are the helper functions used for <strong>maximum likelihood estimation</strong>. The base <code class="docutils literal notranslate"><span class="pre">LeafModule</span></code> handles all remaining functionality.</p>
<p>Custom leaf distributions should follow the structure of a standard PyTorch distribution. Therefore, if your distribution is not derived from an existing PyTorch distribution class, you must implement the distribution logic and all required methods yourself.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">nn</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">spflow.modules.leaves.leaf</span><span class="w"> </span><span class="kn">import</span> <span class="n">LeafModule</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">spflow.utils.leaves</span><span class="w"> </span><span class="kn">import</span> <span class="n">init_parameter</span><span class="p">,</span> <span class="n">_handle_mle_edge_cases</span>


<span class="k">class</span><span class="w"> </span><span class="nc">CustomLeaf</span><span class="p">(</span><span class="n">LeafModule</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Custom distribution leaf module.</span>

<span class="sd">    Parameterized by border.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        border: Border that separates the value interval.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">scope</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_repetitions</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">parameter_fn</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">validate_args</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">border</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Normal distribution.</span>

<span class="sd">        Args:</span>
<span class="sd">            scope: Variable scope (Scope, int, or list[int]).</span>
<span class="sd">            out_channels: Number of output channels (inferred from params if None).</span>
<span class="sd">            num_repetitions: Number of repetitions (for 3D event shapes).</span>
<span class="sd">            parameter_fn: Optional neural network for parameter generation.</span>
<span class="sd">            border: Border tensor that separates the value interval.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">scope</span><span class="o">=</span><span class="n">scope</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="n">out_channels</span><span class="p">,</span>
            <span class="n">num_repetitions</span><span class="o">=</span><span class="n">num_repetitions</span><span class="p">,</span>
            <span class="n">params</span><span class="o">=</span><span class="p">[</span><span class="n">border</span><span class="p">],</span>
            <span class="n">parameter_fn</span><span class="o">=</span><span class="n">parameter_fn</span><span class="p">,</span>
            <span class="n">validate_args</span><span class="o">=</span><span class="n">validate_args</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">border</span> <span class="o">=</span> <span class="n">init_parameter</span><span class="p">(</span><span class="n">param</span><span class="o">=</span><span class="n">border</span><span class="p">,</span> <span class="n">event_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_event_shape</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span>
        <span class="n">border</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">border</span><span class="p">,</span> <span class="o">-</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">border</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">border</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_supported_value</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="mf">0.0</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_torch_distribution_class</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">type</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Normal</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">_CustomLeafDistribution</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;border&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">border</span><span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_compute_parameter_estimates</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">bias_correction</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute raw MLE estimates for normal distribution (without broadcasting).</span>

<span class="sd">        Args:</span>
<span class="sd">            data: Input data tensor.</span>
<span class="sd">            weights: Weight tensor for each data point.</span>
<span class="sd">            bias_correction: Whether to apply bias correction to variance estimate.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dictionary with &#39;loc&#39; and &#39;scale&#39; estimates (shape: out_features).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;border&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">border</span><span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_set_mle_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set MLE-estimated parameters for Normal distribution.</span>

<span class="sd">        Explicitly handles the two parameter types:</span>
<span class="sd">        - loc: Direct nn.Parameter, update .data attribute</span>
<span class="sd">        - scale: Property with setter, calls property setter which updates log_scale</span>

<span class="sd">        Args:</span>
<span class="sd">            params_dict: Dictionary with &#39;loc&#39; and &#39;scale&#39; parameter values.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">border</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">params_dict</span><span class="p">[</span><span class="s2">&quot;border&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">params_dict</span><span class="p">[</span><span class="s2">&quot;scale&quot;</span><span class="p">]</span>  <span class="c1"># Uses property setter</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_mle_update_statistics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">bias_correction</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute weighted mean and standard deviation.</span>

<span class="sd">        Args:</span>
<span class="sd">            data: Input data tensor.</span>
<span class="sd">            weights: Weight tensor for each data point.</span>
<span class="sd">            bias_correction: Whether to apply bias correction to variance estimate.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">estimates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_parameter_estimates</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">bias_correction</span><span class="p">)</span>

        <span class="c1"># Broadcast to event_shape and assign directly</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">border</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_broadcast_to_event_shape</span><span class="p">(</span><span class="n">estimates</span><span class="p">[</span><span class="s2">&quot;border&quot;</span><span class="p">])</span>


<span class="k">class</span><span class="w"> </span><span class="nc">_CustomLeafDistribution</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Custom Hypergeometric distribution implementation.</span>

<span class="sd">    Since PyTorch doesn&#39;t have a built-in Hypergeometric distribution,</span>
<span class="sd">    this class implements the necessary methods for inference and sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">border</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">validate_args</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">border</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">border</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">event_shape</span> <span class="o">=</span> <span class="n">border</span><span class="o">.</span><span class="n">shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validate_args</span> <span class="o">=</span> <span class="n">validate_args</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_clamped_border</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Ensure border stays inside the allowed interval</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">border</span><span class="p">,</span> <span class="o">-</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">check_support</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Hypergeometric support: integer counts within valid borders.</span>

<span class="sd">        Valid range: max(0, n+K-N) &lt;= x &lt;= min(n, K)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">valid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">((</span><span class="n">data</span> <span class="o">&gt;=</span> <span class="o">-</span><span class="mf">10.0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">data</span> <span class="o">&lt;=</span> <span class="mf">10.0</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">valid</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">mode</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the mode of the distribution.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_clamped_border</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute log probability using logarithmic identities to avoid overflow.&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">validate_args</span><span class="p">:</span>
            <span class="n">support_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_support</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="n">border</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_clamped_border</span><span class="p">()</span>

        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">data</span> <span class="o">&gt;=</span> <span class="n">border</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">)),</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">)))</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">support_mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sample `n_samples` draws.</span>
<span class="sd">        Returns shape: (n_samples,)</span>
<span class="sd">        Values are 10 if sample &gt;= border, otherwise 0.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">n_samples</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_samples</span><span class="p">,)</span>

        <span class="c1"># Prepare the tensor to store the samples</span>
        <span class="n">sample_shape</span> <span class="o">=</span> <span class="n">n_samples</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">event_shape</span>

        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">border</span><span class="o">.</span><span class="n">device</span>
        <span class="n">region_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.7</span>
        <span class="n">border</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">border</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span>

        <span class="c1"># Step 2: allocate output</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Step 3: sample each region element-wise</span>
        <span class="c1"># Upper region: U(border[i], high)</span>
        <span class="k">if</span> <span class="n">region_mask</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
            <span class="n">samples</span><span class="p">[</span><span class="n">region_mask</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">border</span><span class="p">[</span><span class="n">region_mask</span><span class="p">]</span>
                <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">border</span><span class="p">[</span><span class="n">region_mask</span><span class="p">])</span> <span class="o">*</span> <span class="p">(</span><span class="mi">10</span> <span class="o">-</span> <span class="n">border</span><span class="p">[</span><span class="n">region_mask</span><span class="p">])</span>
            <span class="p">)</span>

        <span class="c1"># Lower region: U(low, border[i])</span>
        <span class="k">if</span> <span class="p">(</span><span class="o">~</span><span class="n">region_mask</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
            <span class="n">samples</span><span class="p">[</span><span class="o">~</span><span class="n">region_mask</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="o">-</span><span class="mi">10</span>
                <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">border</span><span class="p">[</span><span class="o">~</span><span class="n">region_mask</span><span class="p">])</span> <span class="o">*</span> <span class="p">(</span><span class="n">border</span><span class="p">[</span><span class="o">~</span><span class="n">region_mask</span><span class="p">]</span> <span class="o">-</span> <span class="o">-</span><span class="mi">10</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">samples</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">spflow.meta</span><span class="w"> </span><span class="kn">import</span> <span class="n">Scope</span>
<span class="n">scope</span> <span class="o">=</span> <span class="n">Scope</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>
<span class="n">custom_leaf</span> <span class="o">=</span> <span class="n">CustomLeaf</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="n">scope</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">custom_leaf</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">num_samples</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">custom_leaf</span><span class="o">.</span><span class="n">border</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="o">-</span><span class="mi">10</span> <span class="o">+</span> <span class="mi">20</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ll</span> <span class="o">=</span> <span class="n">custom_leaf</span><span class="o">.</span><span class="n">log_likelihood</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">custom_leaf</span><span class="o">.</span><span class="n">border</span><span class="p">)</span>
<span class="n">ll</span>
<span class="n">custom_leaf</span><span class="o">.</span><span class="n">mode</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor([[[-10.0000],
         [  6.8885]]])
torch.Size([10, 1])
tensor([[  0.4254],
        [ -4.1549],
        [ -5.9391],
        [ -7.1722],
        [-10.0000],
        [ -8.3070],
        [ -8.4731],
        [-10.0000],
        [  7.4759],
        [  2.9279]], grad_fn=&lt;IndexPutBackward0&gt;)
tensor([[ 6.3682],
        [-5.8387],
        [ 3.1316],
        [ 0.5444],
        [-9.9559]])
Parameter containing:
tensor([[[-10.0000],
         [  6.8885]]], requires_grad=True)
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/tmp/ipykernel_2375/528264443.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.border = torch.nn.Parameter(torch.tensor(border))
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor([[[-10.0000],
         [  6.8885]]], grad_fn=&lt;ClampBackward1&gt;)
</pre></div></div>
</div>
</section>
<section id="Intermediate-Modules">
<h2>Intermediate Modules<a class="headerlink" href="#Intermediate-Modules" title="Link to this heading">¶</a></h2>
<p>Each intermediate module receives one or more input modules. Every module in the library is designed to process exactly one input. Therefore, if you want to pass multiple modules as input, you must first concatenate them using the <code class="docutils literal notranslate"><span class="pre">Cat</span></code> module.</p>
<p>All modules must implement the abstract properties and methods defined in the base <code class="docutils literal notranslate"><span class="pre">Module</span></code> class.</p>
<section id="Required-Methods">
<h3>Required Methods<a class="headerlink" href="#Required-Methods" title="Link to this heading">¶</a></h3>
<ol class="arabic simple">
<li><p><strong>log_likelihood</strong></p></li>
<li><p><strong>sample</strong></p></li>
<li><p><strong>marginalize</strong></p></li>
</ol>
</section>
<section id="Required-Properties">
<h3>Required Properties<a class="headerlink" href="#Required-Properties" title="Link to this heading">¶</a></h3>
<ol class="arabic simple">
<li><p><strong>out_features</strong></p></li>
<li><p><strong>out_channels</strong></p></li>
<li><p><strong>feature_to_scope</strong></p></li>
</ol>
<p>Whether these properties can be inferred internally or must be provided explicitly at construction time depends on the module type.</p>
<p>For example, a <strong>Sum</strong> module typically requires additional parameters such as:</p>
<ul class="simple">
<li><p>number of output channels,</p></li>
<li><p>number of repetitions,</p></li>
<li><p>an optional weight tensor,</p></li>
<li><p>the summation dimension.</p></li>
</ul>
<p>A normal <strong>Product</strong> module, on the other hand, always has the same number of output channels as its input, so this value cannot (and does not need to) be specified manually.</p>
</section>
</section>
<hr class="docutils" />
<section id="Example:-Implementing-a-Custom-SquaredSum-Module">
<h2>Example: Implementing a Custom <code class="docutils literal notranslate"><span class="pre">SquaredSum</span></code> Module<a class="headerlink" href="#Example:-Implementing-a-Custom-SquaredSum-Module" title="Link to this heading">¶</a></h2>
<p>In this section we implement a custom <code class="docutils literal notranslate"><span class="pre">SquaredSum</span></code> module. Conceptually, it behaves like a standard sum module, except that it squares its inputs before summing them.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">SquaredSum</span></code> class inherits from the base <code class="docutils literal notranslate"><span class="pre">Module</span></code> class (although inheriting from the existing <code class="docutils literal notranslate"><span class="pre">Sum</span></code> module would also be possible). For clarity, we use the base class as our starting point.</p>
<p>Because this is a sum-like module, we introduce a weight tensor. The most common shape for such weights is:</p>
<p>(out_features, in_channels, out_channels, num_repetitions)</p>
<p>These weights are trainable parameters and therefore must be registered. To avoid unwanted behavior, the module performs several checks to ensure that:</p>
<ul class="simple">
<li><p>the weight tensor has the correct shape,</p></li>
<li><p>all weights are positive,</p></li>
<li><p>and the weights are normalized to sum to 1.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="log_likelihood">
<h2><code class="docutils literal notranslate"><span class="pre">log_likelihood</span></code><a class="headerlink" href="#log_likelihood" title="Link to this heading">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">log_likelihood</span></code> method begins by initializing the module’s cache, which stores computed log-likelihoods for later use (e.g., during sampling).</p>
<p>The log-likelihood produced by any module always has the shape:</p>
<p>(batch_size, out_features, out_channels, num_repetitions)</p>
<p>You can rely on this shape when accessing the output of the input module.</p>
<p>For the <code class="docutils literal notranslate"><span class="pre">SquaredSum</span></code> module, the log-likelihood computation consists of taking a weighted squared sum of the input log-likelihoods. The returned tensor must again match the standard shape:</p>
<p>(batch_size, out_features, out_channels, num_repetitions)</p>
</section>
<hr class="docutils" />
<section id="sample">
<h2><code class="docutils literal notranslate"><span class="pre">sample</span></code><a class="headerlink" href="#sample" title="Link to this heading">¶</a></h2>
<div class="line-block">
<div class="line">Sampling in this library relies on a <strong>sampling context</strong>.</div>
<div class="line">This context is propagated from the root module toward the leaf modules and contains, for each feature, the selected <strong>channel</strong> and <strong>repetition</strong> indices. Each module must implement its own logic for mapping the channel index in the context to the channel index expected by its input module.</div>
</div>
<p>The repetition index is chosen at the root and remains unchanged throughout propagation.</p>
<p>For the <code class="docutils literal notranslate"><span class="pre">SquaredSum</span></code> module, the sampling procedure works as follows:</p>
<ol class="arabic simple">
<li><p>Extract the weight slice associated with the repetition index stored in the context.</p></li>
<li><p>Check whether log-likelihoods are available in the cache for conditional sampling.</p></li>
<li><p>If performing MPE sampling, select the channel indices with the highest logits.</p></li>
<li><p>Otherwise, draw channel indices using a categorical distribution over the logits.</p></li>
<li><p>Update the sampling context with the newly selected indices.</p></li>
<li><p>Pass the updated context to the input module to continue the sampling process.</p></li>
</ol>
</section>
<hr class="docutils" />
<section id="marginalize">
<h2><code class="docutils literal notranslate"><span class="pre">marginalize</span></code><a class="headerlink" href="#marginalize" title="Link to this heading">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">marginalize</span></code> method structurally marginalizes the module by removing the specified random variables from the layer. This process involves not only updating the scope but also adapting any parameters that depend on the number of features in the layer. If the entire scope is marginalized, the method is expected to return <code class="docutils literal notranslate"><span class="pre">None</span></code>. Otherwise, it should return a new module with updated parameters and marginalized input modules.</p>
<p>In the <code class="docutils literal notranslate"><span class="pre">SquaredSum</span></code> layer, we begin by calling <code class="docutils literal notranslate"><span class="pre">marginalize</span></code> on the input module to obtain its marginalized version. Next, we marginalize the sum layer itself using the <code class="docutils literal notranslate"><span class="pre">feature_to_scope</span></code> property, which enables us to remove the appropriate feature column from the weight matrix. We then construct a new <code class="docutils literal notranslate"><span class="pre">SquaredSum</span></code> module using the marginalized input module and the updated weight matrix as the layer’s weights. From these updated weights, the new scope can be derived as described earlier.
Finally, we return the marginalized <code class="docutils literal notranslate"><span class="pre">SquaredSum</span></code> layer.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">spflow.exceptions</span><span class="w"> </span><span class="kn">import</span> <span class="n">InvalidParameterCombinationError</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">spflow.modules.module</span><span class="w"> </span><span class="kn">import</span> <span class="n">Module</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">spflow.modules.ops.cat</span><span class="w"> </span><span class="kn">import</span> <span class="n">Cat</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">spflow.utils.cache</span><span class="w"> </span><span class="kn">import</span> <span class="n">Cache</span><span class="p">,</span> <span class="n">cached</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">spflow.utils.projections</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">proj_convex_to_real</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">spflow.utils.sampling_context</span><span class="w"> </span><span class="kn">import</span> <span class="n">SamplingContext</span><span class="p">,</span> <span class="n">init_default_sampling_context</span>


<span class="k">class</span><span class="w"> </span><span class="nc">SquaredSum</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sum module representing mixture operations in probabilistic circuits.</span>

<span class="sd">    Implements mixture modeling by computing weighted combinations of child distributions.</span>
<span class="sd">    Weights are normalized to sum to one, maintaining valid probability distributions.</span>
<span class="sd">    Supports both single input (mixture over channels) and multiple inputs (mixture</span>
<span class="sd">    over concatenated inputs).</span>

<span class="sd">    Attributes:</span>
<span class="sd">        inputs (Module): Input module(s) to the sum node.</span>
<span class="sd">        sum_dim (int): Dimension over which to sum the inputs.</span>
<span class="sd">        weights (Tensor): Normalized weights for mixture components.</span>
<span class="sd">        logits (Parameter): Unnormalized log-weights for gradient optimization.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Module</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">Module</span><span class="p">],</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_repetitions</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">weights</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sum_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create a Sum module for mixture modeling.</span>

<span class="sd">        Weights are automatically normalized to sum to one using softmax.</span>
<span class="sd">        Multiple inputs are concatenated along dimension 2 internally.</span>

<span class="sd">        Args:</span>
<span class="sd">            inputs (Module | list[Module]): Single module or list of modules to mix.</span>
<span class="sd">            out_channels (int | None, optional): Number of output mixture components.</span>
<span class="sd">                Required if weights not provided.</span>
<span class="sd">            num_repetitions (int | None, optional): Number of repetitions for structured</span>
<span class="sd">                representations. Inferred from weights if not provided.</span>
<span class="sd">            weights (Tensor | list[float] | None, optional): Initial mixture weights.</span>
<span class="sd">                Must have compatible shape with inputs and out_channels.</span>
<span class="sd">            sum_dim (int | None, optional): Dimension over which to sum inputs. Default is 1.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If inputs empty, out_channels &lt; 1, or weights have invalid shape/values.</span>
<span class="sd">            InvalidParameterCombinationError: If both out_channels and weights are specified.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># ========== 1. INPUT VALIDATION ==========</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">inputs</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;Sum&#39; requires at least one input to be specified.&quot;</span><span class="p">)</span>

        <span class="c1"># Convert weights from list to tensor if needed</span>
        <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>

        <span class="c1"># ========== 2. WEIGHTS PARAMETER PROCESSING ==========</span>
        <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Validate mutual exclusivity with out_channels</span>
            <span class="k">if</span> <span class="n">out_channels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">InvalidParameterCombinationError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Cannot specify both &#39;out_channels&#39; and &#39;weights&#39; for &#39;Sum&#39; module.&quot;</span>
                <span class="p">)</span>

            <span class="c1"># Validate num_repetitions compatibility</span>
            <span class="k">if</span> <span class="n">num_repetitions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">num_repetitions</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">num_repetitions</span> <span class="o">!=</span> <span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
                <span class="k">raise</span> <span class="n">InvalidParameterCombinationError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Cannot specify &#39;num_repetitions&#39; that does not match weights shape for &#39;Sum&#39; module. &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Was </span><span class="si">{</span><span class="n">num_repetitions</span><span class="si">}</span><span class="s2"> but weights shape indicates </span><span class="si">{</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>

            <span class="c1"># Reshape weights to canonical 4D form: (out_features, in_channels, out_channels, num_repetitions)</span>
            <span class="n">weight_dim</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">weight_dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">weight_dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">elif</span> <span class="n">weight_dim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
                <span class="k">pass</span>  <span class="c1"># Already 3D, will add repetition dimension below</span>
            <span class="k">elif</span> <span class="n">weight_dim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
                <span class="k">pass</span>  <span class="c1"># Already 4D</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Weights for &#39;Sum&#39; must be a 1D, 2D, 3D, or 4D tensor but was </span><span class="si">{</span><span class="n">weight_dim</span><span class="si">}</span><span class="s2">D.&quot;</span>
                <span class="p">)</span>

            <span class="c1"># Derive configuration from weights shape</span>
            <span class="n">out_channels</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
            <span class="n">num_repetitions</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>

        <span class="c1"># ========== 3. CONFIGURATION VALIDATION ==========</span>
        <span class="k">if</span> <span class="n">out_channels</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Number of nodes for &#39;Sum&#39; must be greater of equal to 1 but was </span><span class="si">{</span><span class="n">out_channels</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Validate sum_dim compatibility with weights dimensionality</span>
        <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">max_sum_dim</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">sum_dim</span> <span class="o">&gt;</span> <span class="n">max_sum_dim</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;When providing </span><span class="si">{</span><span class="n">weights</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">D weights, &#39;sum_dim&#39; must be at most </span><span class="si">{</span><span class="n">max_sum_dim</span><span class="si">}</span><span class="s2"> but was </span><span class="si">{</span><span class="n">sum_dim</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>

        <span class="c1"># ========== 4. INPUT MODULE SETUP ==========</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">Cat</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">sum_dim</span> <span class="o">=</span> <span class="n">sum_dim</span>

        <span class="c1"># ========== 5. ATTRIBUTE INITIALIZATION ==========</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_out_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="o">.</span><span class="n">out_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_in_channels_total</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="o">.</span><span class="n">out_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_out_channels_total</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_repetitions</span> <span class="o">=</span> <span class="n">num_repetitions</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights_shape</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_out_features</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_in_channels_total</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_out_channels_total</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_repetitions</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">scope</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="o">.</span><span class="n">scope</span>

        <span class="c1"># ========== 6. WEIGHT INITIALIZATION &amp; PARAMETER REGISTRATION ==========</span>
        <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Initialize weights randomly with small epsilon to avoid zeros</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_shape</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-08</span>
            <span class="c1"># Normalize to sum to one along sum_dim</span>
            <span class="n">weights</span> <span class="o">/=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sum_dim</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Register parameter for unnormalized log-probabilities</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">()</span>

        <span class="c1"># Set weights (converts to logits internally via property setter)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">feature_to_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="o">.</span><span class="n">feature_to_scope</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">out_features</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_out_features</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">out_channels</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_out_channels_total</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">log_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the log weights of all nodes as a tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor: Log weights normalized to sum to one.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># project auxiliary weights onto weights that sum up to one</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sum_dim</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the weights of all nodes as a tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor: Weights normalized to sum to one.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># project auxiliary weights onto weights that sum up to one</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sum_dim</span><span class="p">)</span>

    <span class="nd">@weights</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">values</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set weights of all nodes.</span>

<span class="sd">        Args:</span>
<span class="sd">            values: Tensor containing weights for each input and node.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If weights have invalid shape, contain non-positive values,</span>
<span class="sd">                or do not sum to one.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">values</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Invalid shape for weights: Was </span><span class="si">{</span><span class="n">values</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> but expected </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_shape</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">values</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Weights for &#39;Sum&#39; must be all positive.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sum_dim</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Weights for &#39;Sum&#39; must sum up to one.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logits</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">proj_convex_to_real</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>

    <span class="nd">@log_weights</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">log_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">values</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set log weights of all nodes.</span>

<span class="sd">        Args:</span>
<span class="sd">            values: Tensor containing log weights for each input and node.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If log weights have invalid shape.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">values</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid shape for weights: </span><span class="si">{</span><span class="n">values</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logits</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">values</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">extra_repr</span><span class="p">()</span><span class="si">}</span><span class="s2">, weights=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_shape</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="nd">@cached</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">log_likelihood</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">cache</span><span class="p">:</span> <span class="n">Cache</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute log likelihood P(data | module).</span>

<span class="sd">        Computes log likelihood using logsumexp for numerical stability.</span>
<span class="sd">        Results are cached for parameter learning algorithms.</span>

<span class="sd">        Args:</span>
<span class="sd">            data: Input data of shape (batch_size, num_features).</span>
<span class="sd">                NaN values indicate evidence for conditional computation.</span>
<span class="sd">            cache: Cache for intermediate computations. Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor: Log-likelihood of shape (batch_size, num_features, out_channels)</span>
<span class="sd">                or (batch_size, num_features, out_channels, num_repetitions).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">cache</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">cache</span> <span class="o">=</span> <span class="n">Cache</span><span class="p">()</span>

        <span class="c1"># Get input log-likelihoods</span>
        <span class="n">ll</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="o">.</span><span class="n">log_likelihood</span><span class="p">(</span>
            <span class="n">data</span><span class="p">,</span>
            <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">ll</span> <span class="o">=</span> <span class="n">ll</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

        <span class="n">squared_ll</span> <span class="o">=</span> <span class="n">ll</span> <span class="o">*</span> <span class="mi">2</span><span class="c1"># shape: (B, F, input_OC, R)</span>

        <span class="n">log_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># shape: (1, F, IC, OC, R)</span>

        <span class="c1"># Weighted log-likelihoods</span>
        <span class="n">weighted_lls</span> <span class="o">=</span> <span class="n">squared_ll</span> <span class="o">+</span> <span class="n">log_weights</span>  <span class="c1"># shape: (B, F, IC, OC, R)</span>

        <span class="c1"># Sum over input channels (sum_dim + 1 since here the batch dimension is the first dimension)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">weighted_lls</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sum_dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_repetitions</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">result</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">data</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">is_mpe</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">cache</span><span class="p">:</span> <span class="n">Cache</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sampling_ctx</span><span class="p">:</span> <span class="n">SamplingContext</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate samples from sum module.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_samples: Number of samples to generate.</span>
<span class="sd">            data: Data tensor with NaN values to fill with samples.</span>
<span class="sd">            is_mpe: Whether to perform maximum a posteriori estimation.</span>
<span class="sd">            cache: Optional cache dictionary.</span>
<span class="sd">            sampling_ctx: Optional sampling context.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor: Sampled values.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">cache</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">cache</span> <span class="o">=</span> <span class="n">Cache</span><span class="p">()</span>

        <span class="c1"># Handle num_samples case (create empty data tensor)</span>
        <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">num_samples</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">num_samples</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scope</span><span class="o">.</span><span class="n">query</span><span class="p">)),</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;nan&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Initialize sampling context if not provided</span>
        <span class="n">sampling_ctx</span> <span class="o">=</span> <span class="n">init_default_sampling_context</span><span class="p">(</span><span class="n">sampling_ctx</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Index into the correct weight channels given by parent module</span>
        <span class="k">if</span> <span class="n">sampling_ctx</span><span class="o">.</span><span class="n">repetition_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logits</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span>
                <span class="n">sampling_ctx</span><span class="o">.</span><span class="n">channel_index</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span>
            <span class="p">)</span>  <span class="c1"># shape [b , n_features , in_c, out_c, r]</span>

            <span class="n">indices</span> <span class="o">=</span> <span class="n">sampling_ctx</span><span class="o">.</span><span class="n">repetition_idx</span>  <span class="c1"># Shape (30000, 1)</span>

            <span class="c1"># Use gather to select the correct repetition</span>
            <span class="c1"># Repeat indices to match the target dimension for gathering</span>
            <span class="n">in_channels_total</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span>
                <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">in_channels_total</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span>
            <span class="p">)</span>
            <span class="c1"># Gather the logits based on the repetition indices</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">indices</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_repetitions</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;sampling_ctx.repetition_idx must be provided when sampling from a module with &quot;</span>
                    <span class="s2">&quot;num_repetitions &gt; 1.&quot;</span>
                <span class="p">)</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logits</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="c1"># Select the 0th repetition</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># Make space for the batch</span>

            <span class="c1"># Expand to batch size</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">sampling_ctx</span><span class="o">.</span><span class="n">channel_index</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">idxs</span> <span class="o">=</span> <span class="n">sampling_ctx</span><span class="o">.</span><span class="n">channel_index</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="n">in_channels_total</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">idxs</span> <span class="o">=</span> <span class="n">idxs</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">in_channels_total</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Gather the logits based on the channel indices</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">idxs</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

        <span class="c1"># Check if evidence is given (cached log-likelihoods)</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="s2">&quot;log_likelihood&quot;</span> <span class="ow">in</span> <span class="n">cache</span>
            <span class="ow">and</span> <span class="n">cache</span><span class="p">[</span><span class="s2">&quot;log_likelihood&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="c1"># Get the log likelihoods from the cache</span>
            <span class="n">input_lls</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="s2">&quot;log_likelihood&quot;</span><span class="p">][</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">sampling_ctx</span><span class="o">.</span><span class="n">repetition_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">indices</span> <span class="o">=</span> <span class="n">sampling_ctx</span><span class="o">.</span><span class="n">repetition_idx</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span>
                    <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_lls</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">input_lls</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span>
                <span class="p">)</span>

                <span class="c1"># Use gather to select the correct repetition</span>
                <span class="n">input_lls</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">input_lls</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">indices</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

                <span class="n">log_prior</span> <span class="o">=</span> <span class="n">logits</span>
                <span class="n">log_posterior</span> <span class="o">=</span> <span class="n">log_prior</span> <span class="o">+</span> <span class="n">input_lls</span>
                <span class="n">log_posterior</span> <span class="o">=</span> <span class="n">log_posterior</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
                <span class="n">logits</span> <span class="o">=</span> <span class="n">log_posterior</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">log_prior</span> <span class="o">=</span> <span class="n">logits</span>
                <span class="n">log_posterior</span> <span class="o">=</span> <span class="n">log_prior</span> <span class="o">+</span> <span class="n">input_lls</span>
                <span class="n">log_posterior</span> <span class="o">=</span> <span class="n">log_posterior</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
                <span class="n">logits</span> <span class="o">=</span> <span class="n">log_posterior</span>

        <span class="c1"># Sample from categorical distribution defined by weights to obtain indices into input channels</span>
        <span class="k">if</span> <span class="n">is_mpe</span><span class="p">:</span>
            <span class="c1"># Take the argmax of the logits to obtain the most probable index</span>
            <span class="n">sampling_ctx</span><span class="o">.</span><span class="n">channel_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Sample from categorical distribution defined by weights to obtain indices into input channels</span>
            <span class="n">sampling_ctx</span><span class="o">.</span><span class="n">channel_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

        <span class="c1"># Sample from input module</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
            <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
            <span class="n">is_mpe</span><span class="o">=</span><span class="n">is_mpe</span><span class="p">,</span>
            <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
            <span class="n">sampling_ctx</span><span class="o">=</span><span class="n">sampling_ctx</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">data</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">expectation_maximization</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">cache</span><span class="p">:</span> <span class="n">Cache</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Perform expectation-maximization step.</span>

<span class="sd">        Args:</span>
<span class="sd">            data: Input data tensor.</span>
<span class="sd">            cache: Optional cache dictionary with log-likelihoods.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If required log-likelihoods are not found in cache.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">cache</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">cache</span> <span class="o">=</span> <span class="n">Cache</span><span class="p">()</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="c1"># ----- expectation step -----</span>

            <span class="c1"># Get input LLs from cache</span>
            <span class="n">input_lls</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="s2">&quot;log_likelihood&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">input_lls</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input log-likelihoods not found in cache. Call log_likelihood first.&quot;</span><span class="p">)</span>

            <span class="c1"># Get module lls from cache</span>
            <span class="n">module_lls</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="s2">&quot;log_likelihood&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">module_lls</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Module log-likelihoods not found in cache. Call log_likelihood first.&quot;</span><span class="p">)</span>

            <span class="n">log_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">log_grads</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">module_lls</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">input_lls</span> <span class="o">=</span> <span class="n">input_lls</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
            <span class="n">module_lls</span> <span class="o">=</span> <span class="n">module_lls</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

            <span class="n">log_expectations</span> <span class="o">=</span> <span class="n">log_weights</span> <span class="o">+</span> <span class="n">log_grads</span> <span class="o">+</span> <span class="n">input_lls</span> <span class="o">-</span> <span class="n">module_lls</span>
            <span class="n">log_expectations</span> <span class="o">=</span> <span class="n">log_expectations</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Sum over batch dimension</span>
            <span class="n">log_expectations</span> <span class="o">=</span> <span class="n">log_expectations</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sum_dim</span><span class="p">)</span>  <span class="c1"># Normalize</span>

            <span class="c1"># ----- maximization step -----</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_weights</span> <span class="o">=</span> <span class="n">log_expectations</span>

        <span class="c1"># Recursively call EM on inputs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="o">.</span><span class="n">expectation_maximization</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">maximum_likelihood_estimation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">weights</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cache</span><span class="p">:</span> <span class="n">Cache</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Update parameters via maximum likelihood estimation.</span>

<span class="sd">        For Sum modules, this is equivalent to EM.</span>

<span class="sd">        Args:</span>
<span class="sd">            data: Input data tensor.</span>
<span class="sd">            weights: Optional sample weights (currently unused).</span>
<span class="sd">            cache: Optional cache dictionary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expectation_maximization</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">marginalize</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">marg_rvs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">prune</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">cache</span><span class="p">:</span> <span class="n">Cache</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SquaredSum</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Marginalize out specified random variables.</span>

<span class="sd">        Args:</span>
<span class="sd">            marg_rvs: List of random variables to marginalize.</span>
<span class="sd">            prune: Whether to prune the module.</span>
<span class="sd">            cache: Optional cache dictionary.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Marginalized Sum module or None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">cache</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">cache</span> <span class="o">=</span> <span class="n">Cache</span><span class="p">()</span>

        <span class="c1"># compute module scope (same for all outputs)</span>
        <span class="n">module_scope</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scope</span>
        <span class="n">marg_input</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">mutual_rvs</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">module_scope</span><span class="o">.</span><span class="n">query</span><span class="p">)</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">marg_rvs</span><span class="p">))</span>
        <span class="n">module_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>

        <span class="c1"># module scope is being fully marginalized over</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">mutual_rvs</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">module_scope</span><span class="o">.</span><span class="n">query</span><span class="p">):</span>
            <span class="c1"># passing this loop means marginalizing over the whole scope of this branch</span>
            <span class="k">return</span> <span class="kc">None</span>

        <span class="c1"># node scope is being partially marginalized</span>
        <span class="k">elif</span> <span class="n">mutual_rvs</span><span class="p">:</span>
            <span class="c1"># marginalize input modules</span>
            <span class="n">marg_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="o">.</span><span class="n">marginalize</span><span class="p">(</span><span class="n">marg_rvs</span><span class="p">,</span> <span class="n">prune</span><span class="o">=</span><span class="n">prune</span><span class="p">,</span> <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">)</span>

            <span class="c1"># if marginalized input is not None</span>
            <span class="k">if</span> <span class="n">marg_input</span><span class="p">:</span>
                <span class="c1"># Apply mask to weights per-repetition</span>
                <span class="n">masked_weights_list</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_repetitions</span><span class="p">):</span>
                    <span class="n">feature_to_scope_r</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="o">.</span><span class="n">feature_to_scope</span><span class="p">[:,</span> <span class="n">r</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                    <span class="c1"># remove mutual_rvs from feature_to_scope list</span>
                    <span class="k">for</span> <span class="n">rv</span> <span class="ow">in</span> <span class="n">mutual_rvs</span><span class="p">:</span>
                        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">scope</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">feature_to_scope_r</span><span class="p">):</span>
                            <span class="k">if</span> <span class="n">scope</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                                <span class="k">if</span> <span class="n">rv</span> <span class="ow">in</span> <span class="n">scope</span><span class="o">.</span><span class="n">query</span><span class="p">:</span>
                                    <span class="n">feature_to_scope_r</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">scope</span><span class="o">.</span><span class="n">remove_from_query</span><span class="p">(</span><span class="n">rv</span><span class="p">)</span>

                    <span class="c1"># construct mask with empty scopes</span>
                    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="ow">not</span> <span class="n">scope</span><span class="o">.</span><span class="n">empty</span><span class="p">()</span> <span class="k">for</span> <span class="n">scope</span> <span class="ow">in</span> <span class="n">feature_to_scope_r</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>

                    <span class="c1"># Apply mask to weights for this repetition: (out_features, in_channels, out_channels)</span>
                    <span class="n">masked_weights_r</span> <span class="o">=</span> <span class="n">module_weights</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">r</span><span class="p">][</span><span class="n">mask</span><span class="p">]</span>
                    <span class="n">masked_weights_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">masked_weights_r</span><span class="p">)</span>

                <span class="c1"># Stack weights back along the repetition dimension</span>
                <span class="c1"># Handle different repetition counts if needed</span>
                <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">masked_weights_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">masked_weights_list</span><span class="p">):</span>
                    <span class="c1"># All repetitions have same number of features, can stack directly</span>
                    <span class="n">module_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">masked_weights_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Features differ across repetitions - this shouldn&#39;t happen in practice</span>
                    <span class="c1"># but handle gracefully by keeping the largest</span>
                    <span class="n">max_features</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">masked_weights_list</span><span class="p">)</span>
                    <span class="n">padded_list</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">masked_weights_list</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">max_features</span><span class="p">:</span>
                            <span class="n">padding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                                <span class="n">max_features</span> <span class="o">-</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                                <span class="n">device</span><span class="o">=</span><span class="n">w</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">w</span><span class="o">.</span><span class="n">dtype</span>
                            <span class="p">)</span>
                            <span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">w</span><span class="p">,</span> <span class="n">padding</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                        <span class="n">padded_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
                    <span class="n">module_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">padded_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">marg_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span>

        <span class="k">if</span> <span class="n">marg_input</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">SquaredSum</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">marg_input</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">module_weights</span><span class="p">,</span> <span class="n">sum_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sum_dim</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="Split-Modules">
<h2>Split Modules<a class="headerlink" href="#Split-Modules" title="Link to this heading">¶</a></h2>
<p>At the end of this guide, we take a look at a rather unique type of intermediate module: the <strong>Split module</strong>. A Split module receives a single module as input and produces multiple outputs by splitting it in a defined way. This becomes relevant whenever a module requires multiple inputs—for example, the elementwise product module, which computes the elementwise product of two or more input modules.</p>
<p>Since there are many possible ways to split a module, the following explanation provides guidance on how to create your own Split module adjusted to your specific splitting strategy.</p>
<p>Split modules inherit from the base <code class="docutils literal notranslate"><span class="pre">Split</span></code> class, where <code class="docutils literal notranslate"><span class="pre">out_features</span></code> and <code class="docutils literal notranslate"><span class="pre">out_channels</span></code> are already defined. Conceptually, a Split module represents a different <em>view</em> of an existing module. Therefore, <code class="docutils literal notranslate"><span class="pre">out_features</span></code> and <code class="docutils literal notranslate"><span class="pre">out_channels</span></code> always match those of the input module. However, you must implement the mapping from feature index to scope in the <code class="docutils literal notranslate"><span class="pre">feature_to_scope</span></code> property. And because Split modules only provide an alternative view, the only method that must be implemented is
<code class="docutils literal notranslate"><span class="pre">log_likelihood</span></code>.</p>
<p>A Split module takes:</p>
<ul class="simple">
<li><p>a single input module,</p></li>
<li><p>the dimension along which the split should occur,</p></li>
<li><p>and the number of splits, which determines how many groups the input module should be divided into.</p></li>
</ul>
<p>In the example below, we implement a <strong>pairwise alternating split</strong>. For instance, if we split along the feature dimension into two groups and the features are:</p>
<p>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</p>
<p>then the resulting split would be:</p>
<ul class="simple">
<li><p>Group 1: <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">4,</span> <span class="pre">5,</span> <span class="pre">8,</span> <span class="pre">9]</span></code></p></li>
<li><p>Group 2: <code class="docutils literal notranslate"><span class="pre">[2,</span> <span class="pre">3,</span> <span class="pre">6,</span> <span class="pre">7]</span></code></p></li>
</ul>
<p>For efficient evaluation of <code class="docutils literal notranslate"><span class="pre">log_likelihood</span></code>, the most common numbers of splits (2 and 3) are implemented in a hard-coded fashion. Other numbers of splits are handled by constructing a mask during initialization.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;Alternating splitting operation for tensor partitioning.</span>

<span class="sd">Distributes features in an alternating pattern across splits using modulo</span>
<span class="sd">arithmetic. Promotes feature diversity across branches. Used in RAT-SPN</span>
<span class="sd">and similar architectures.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">spflow.meta.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">Scope</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">spflow.modules.module</span><span class="w"> </span><span class="kn">import</span> <span class="n">Module</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">spflow.modules.ops.split</span><span class="w"> </span><span class="kn">import</span> <span class="n">Split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">spflow.utils.cache</span><span class="w"> </span><span class="kn">import</span> <span class="n">Cache</span><span class="p">,</span> <span class="n">cached</span>


<span class="k">class</span><span class="w"> </span><span class="nc">SplitAlternatePairwise</span><span class="p">(</span><span class="n">Split</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Split operation using pairwise alternating feature distribution.</span>

<span class="sd">    Distributes features pairwise: features (0,1) go to split 0, (2,3) go to split 1,</span>
<span class="sd">    (4,5) go to split 2, then repeating modulo num_splits.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_splits</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="mi">2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">num_splits</span><span class="o">=</span><span class="n">num_splits</span><span class="p">)</span>

        <span class="n">num_f</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">out_features</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">device</span>

        <span class="c1"># Pairwise alternating index assignment:</span>
        <span class="c1"># For feature i, compute which pair it&#39;s in (i // 2), then modulo num_splits</span>
        <span class="n">pair_ids</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_f</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span> <span class="o">%</span> <span class="n">num_splits</span>

        <span class="c1"># Create masks for each split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">split_masks</span> <span class="o">=</span> <span class="p">[</span><span class="n">pair_ids</span> <span class="o">==</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_splits</span><span class="p">)]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">extra_repr</span><span class="p">()</span><span class="si">}</span><span class="s2">, dim=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">feature_to_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">Scope</span><span class="p">]:</span>
        <span class="n">scopes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">feature_to_scope</span>
        <span class="n">feature_to_scope</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># pairwise selection: (0,1)-&gt;split0, (2,3)-&gt;split1, ...</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_splits</span><span class="p">):</span>
            <span class="n">sub_scopes</span> <span class="o">=</span> <span class="p">[</span><span class="n">scopes</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">scopes</span><span class="p">))</span>
                          <span class="k">if</span> <span class="p">((</span><span class="n">j</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_splits</span><span class="p">)</span> <span class="o">==</span> <span class="n">i</span><span class="p">]</span>
            <span class="n">feature_to_scope</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sub_scopes</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">feature_to_scope</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">split_masks</span> <span class="o">=</span> <span class="p">[</span><span class="n">fn</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span> <span class="k">for</span> <span class="n">mask</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_masks</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="nd">@cached</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">cache</span><span class="p">:</span> <span class="n">Cache</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">lls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">log_likelihood</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_splits</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">lls</span><span class="p">]</span>

        <span class="c1"># Hard-code pairwise versions for common cases</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_splits</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="c1"># pairs: (0,1)-&gt;0, (2,3)-&gt;1, ...</span>
            <span class="k">return</span> <span class="p">[</span>
                <span class="n">lls</span><span class="p">[:,</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">lls</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">],</span> <span class="o">...</span><span class="p">],</span>
                <span class="n">lls</span><span class="p">[:,</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">lls</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span><span class="p">],</span> <span class="o">...</span><span class="p">]</span>
            <span class="p">]</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_splits</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span>
                <span class="n">lls</span><span class="p">[:,</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">lls</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span> <span class="o">%</span> <span class="mi">3</span> <span class="o">==</span> <span class="mi">0</span><span class="p">],</span> <span class="o">...</span><span class="p">],</span>
                <span class="n">lls</span><span class="p">[:,</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">lls</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span> <span class="o">%</span> <span class="mi">3</span> <span class="o">==</span> <span class="mi">1</span><span class="p">],</span> <span class="o">...</span><span class="p">],</span>
                <span class="n">lls</span><span class="p">[:,</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">lls</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span> <span class="o">%</span> <span class="mi">3</span> <span class="o">==</span> <span class="mi">2</span><span class="p">],</span> <span class="o">...</span><span class="p">]</span>
            <span class="p">]</span>

        <span class="c1"># General fallback: use masks computed in __init__</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">lls</span><span class="p">[:,</span> <span class="n">mask</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="k">for</span> <span class="n">mask</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_masks</span><span class="p">]</span>
<br/></pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tests.utils.leaves</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_normal_leaf</span><span class="p">,</span> <span class="n">make_normal_data</span>
<span class="n">out_features</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">out_channels</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">num_reps</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">num_splits</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">scope</span> <span class="o">=</span> <span class="n">Scope</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)))</span>

<span class="n">inputs_a</span> <span class="o">=</span> <span class="n">make_normal_leaf</span><span class="p">(</span><span class="n">scope</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">num_repetitions</span><span class="o">=</span><span class="n">num_reps</span><span class="p">)</span>

<span class="n">module</span> <span class="o">=</span> <span class="n">SplitAlternatePairwise</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs_a</span><span class="p">,</span> <span class="n">num_splits</span><span class="o">=</span><span class="n">num_splits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">make_normal_data</span><span class="p">(</span><span class="n">out_features</span><span class="o">=</span><span class="n">module</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">lls</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">log_likelihood</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">feature_to_scope</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">feature_to_scope</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<br/><br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">AttributeError</span>                            Traceback (most recent call last)
<span class="ansi-cyan-fg">Cell</span><span class="ansi-cyan-fg"> </span><span class="ansi-green-fg">In[5]</span><span class="ansi-green-fg">, line 10</span>
<span class="ansi-green-fg">      6</span> scope = Scope(<span style="color: rgb(0,135,0)">list</span>(<span style="color: rgb(0,135,0)">range</span>(<span class="ansi-green-fg">0</span>, out_features)))
<span class="ansi-green-fg">      8</span> inputs_a = make_normal_leaf(scope, out_channels=out_channels, num_repetitions=num_reps)
<span class="ansi-green-fg">---&gt; </span><span class="ansi-green-fg">10</span> module = <span class="ansi-yellow-bg">SplitAlternatePairwise</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">inputs</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">inputs_a</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">num_splits</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">num_splits</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">dim</span><span class="ansi-yellow-bg">=</span><span class="ansi-green-fg ansi-yellow-bg">1</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg">     12</span> data = make_normal_data(out_features=module.out_features, num_samples=<span class="ansi-green-fg">1</span>)
<span class="ansi-green-fg">     13</span> lls = module.log_likelihood(data)

<span class="ansi-cyan-fg">Cell</span><span class="ansi-cyan-fg"> </span><span class="ansi-green-fg">In[4]</span><span class="ansi-green-fg">, line 29</span>, in <span class="ansi-cyan-fg">SplitAlternatePairwise.__init__</span><span class="ansi-blue-fg">(self, inputs, dim, num_splits)</span>
<span class="ansi-green-fg">     26</span> <span class="ansi-bold" style="color: rgb(0,135,0)">def</span><span style="color: rgb(188,188,188)"> </span><span class="ansi-blue-fg">__init__</span>(<span style="color: rgb(0,135,0)">self</span>, inputs: Module, dim: <span style="color: rgb(0,135,0)">int</span> = <span class="ansi-green-fg">1</span>, num_splits: <span style="color: rgb(0,135,0)">int</span> | <span class="ansi-bold" style="color: rgb(0,135,0)">None</span> = <span class="ansi-green-fg">2</span>):
<span class="ansi-green-fg">     27</span>     <span style="color: rgb(0,135,0)">super</span>().<span class="ansi-blue-fg">__init__</span>(inputs=inputs, dim=dim, num_splits=num_splits)
<span class="ansi-green-fg">---&gt; </span><span class="ansi-green-fg">29</span>     num_f = <span class="ansi-yellow-bg">inputs</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">out_features</span>
<span class="ansi-green-fg">     30</span>     device = inputs.device
<span class="ansi-green-fg">     32</span>     <span style="color: rgb(95,135,135)"># Pairwise alternating index assignment:</span>
<span class="ansi-green-fg">     33</span>     <span style="color: rgb(95,135,135)"># For feature i, compute which pair it&#39;s in (i // 2), then modulo num_splits</span>

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/torch/nn/modules/module.py:1964</span>, in <span class="ansi-cyan-fg">Module.__getattr__</span><span class="ansi-blue-fg">(self, name)</span>
<span class="ansi-green-fg">   1962</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> name <span class="ansi-bold" style="color: rgb(175,0,255)">in</span> modules:
<span class="ansi-green-fg">   1963</span>         <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> modules[name]
<span class="ansi-green-fg">-&gt; </span><span class="ansi-green-fg">1964</span> <span class="ansi-bold" style="color: rgb(0,135,0)">raise</span> <span class="ansi-bold" style="color: rgb(215,95,95)">AttributeError</span>(
<span class="ansi-green-fg">   1965</span>     <span class="ansi-yellow-fg">f</span><span class="ansi-yellow-fg">&#34;</span><span class="ansi-yellow-fg">&#39;</span><span class="ansi-bold" style="color: rgb(175,95,135)">{</span><span style="color: rgb(0,135,0)">type</span>(<span style="color: rgb(0,135,0)">self</span>).<span class="ansi-blue-fg">__name__</span><span class="ansi-bold" style="color: rgb(175,95,135)">}</span><span class="ansi-yellow-fg">&#39;</span><span class="ansi-yellow-fg"> object has no attribute </span><span class="ansi-yellow-fg">&#39;</span><span class="ansi-bold" style="color: rgb(175,95,135)">{</span>name<span class="ansi-bold" style="color: rgb(175,95,135)">}</span><span class="ansi-yellow-fg">&#39;</span><span class="ansi-yellow-fg">&#34;</span>
<span class="ansi-green-fg">   1966</span> )

<span class="ansi-red-fg">AttributeError</span>: &#39;DummyLeaf&#39; object has no attribute &#39;out_features&#39;
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="../api/index.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">API Documentation</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="user_guide.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">User Guide</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2025, SPFlow Contributors
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Developer Guide</a><ul>
<li><a class="reference internal" href="#Leaf-Modules">Leaf Modules</a></li>
<li><a class="reference internal" href="#Intermediate-Modules">Intermediate Modules</a><ul>
<li><a class="reference internal" href="#Required-Methods">Required Methods</a></li>
<li><a class="reference internal" href="#Required-Properties">Required Properties</a></li>
</ul>
</li>
<li><a class="reference internal" href="#Example:-Implementing-a-Custom-SquaredSum-Module">Example: Implementing a Custom <code class="docutils literal notranslate"><span class="pre">SquaredSum</span></code> Module</a></li>
<li><a class="reference internal" href="#log_likelihood"><code class="docutils literal notranslate"><span class="pre">log_likelihood</span></code></a></li>
<li><a class="reference internal" href="#sample"><code class="docutils literal notranslate"><span class="pre">sample</span></code></a></li>
<li><a class="reference internal" href="#marginalize"><code class="docutils literal notranslate"><span class="pre">marginalize</span></code></a></li>
<li><a class="reference internal" href="#Split-Modules">Split Modules</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=8d563738"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=46bd48cc"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </body>
</html>