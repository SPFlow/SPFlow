{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPFlow 1.0.0 Design Concept\n",
    "\n",
    "Design basis:\n",
    "* Easy-to use and extend\n",
    "<br>\n",
    "\n",
    "* Functional design (using dispatch)\n",
    "<br>\n",
    "\n",
    "* Modular building blocks that can be stacked and nested\n",
    "    * Allows for quick design of large models\n",
    "    * Allows to combine existing or extend existing models\n",
    "<br>\n",
    "\n",
    "\n",
    "* Support multiple back-end with one-to-one mappings between backends\n",
    "    * Allows for easy model sharing and conversion to favorite back-end\n",
    "    * Base back-end should use explicit node modules as lowest basic blocks\n",
    "        * E.g allows alternative node-wise evaluations (e.g. for computing p-values)\n",
    "    * Other back-ends may use optimized modules (i.e. implicit nodes)\n",
    "        * Still need to be mappable to base-backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![overview](uml/all.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Tuple, Set, Dict, Union, Optional\n",
    "from abc import ABC, abstractmethod, abstractproperty\n",
    "from dataclasses import dataclass\n",
    "import itertools\n",
    "from multipledispatch import dispatch\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NetworkType Class\n",
    "\n",
    "* All network types (e.g. `SPN`, `BN`, ...) inherit from it\n",
    "* Network types do not need to actually implement anything\n",
    "* Simply needed to dispatch on network type objects (e.g. for computing scopes, likelihoods, sampling etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![network_type](uml/network_type.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkType(ABC):\n",
    "    \"\"\"Abstract base class for network types.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPN Network Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPN(NetworkType):\n",
    "    \"\"\"Sum-Product Network (SPN) network type.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scope Class\n",
    "\n",
    "Attributes:\n",
    "* `scope`: set of indices (scope)\n",
    "* `ntype`: network type this scope is in reference to\n",
    "\n",
    "The class can, for example, overwride `__add__` operator to merge scopes based on their network types.\n",
    "\n",
    "<b>TODO:</b>\n",
    "* Network types and modules may have to check whether or not this merging is valid (e.g. sum- vs product-nodes)\n",
    "* Conditional/evidence variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![scope](uml/scope.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scope:\n",
    "    \"\"\"Class representing Variable scopes.\"\"\"\n",
    "    def __init__(self, ntype: NetworkType, variables: Set[int]) -> None:\n",
    "        self.variables = set(variables)\n",
    "        self.network_type = ntype\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Scope({self.variables})\"\n",
    "    \n",
    "    def __add__(self, other: \"Scope\") -> \"Scope\":\n",
    "        \"Dispatches scope merging based on network types.\"\n",
    "        return merge_scope(self, other)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging scopes is automatically dispatched to merging scopes based on their respective network type contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dispatch(Scope, Scope)\n",
    "def merge_scope(scope1: Scope, scope2: Scope) -> Scope:\n",
    "    \"\"\"Generic intermediate function to dispatch merging based on network types.\"\"\"\n",
    "    return merge_scope(scope1.network_type, scope1, scope2.network_type, scope2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To register a new merge operation (e.g. between two `SPN` scopes): dispatch appropriate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dispatch(SPN, Scope, SPN, Scope)\n",
    "def merge_scope(ntype1: SPN, scope1: Scope, ntype2: SPN, scope2: Scope) -> Scope:\n",
    "    \"\"\"Merges two SPN scopes.\"\"\"\n",
    "    return Scope(SPN(), set.union(scope1.variables, scope2.variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scope 1: {0, 1}\n"
     ]
    }
   ],
   "source": [
    "s1 = Scope(SPN(), [0,1])\n",
    "print(f\"Scope 1: {s1.variables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scope 2: {1, 2, 3}\n"
     ]
    }
   ],
   "source": [
    "s2 = Scope(SPN(), [1,2,3])\n",
    "print(f\"Scope 2: {s2.variables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged scope: {0, 1, 2, 3}\n"
     ]
    }
   ],
   "source": [
    "s = s1+s2\n",
    "print(f\"Merged scope: {s.variables}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scope Array\n",
    "\n",
    "Array class inheriting from `np.ndarray`. Alterantively one can just pass `dtype=Scope` as an argument to `np.ndarray`.\n",
    "\n",
    "Can store scopes with different scope length in an array format that can easily be propagated through a model (e.g. similar to likelihood values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScopeArray(np.ndarray):\n",
    "    \"\"\"Numpy array with Scope object elements.\"\"\"\n",
    "    def __new__(cls, data) -> np.ndarray:\n",
    "        return np.array(data, dtype=Scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Scope({0, 1}), Scope({1, 2, 3})], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ScopeArray([s1, s2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also allows to use appropriate numpy-operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Scope({0, 1, 2, 3})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ScopeArray([s1, s2]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Class\n",
    "\n",
    "Every module inherits from this class.\n",
    "\n",
    "Each module must also implement `__len__(self)` to return the number of (implicit or explicit) output node of this module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![module](uml/module.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(ABC):\n",
    "    \"\"\"Abstract base class for modules.\n",
    "    \n",
    "    Args:\n",
    "        children: list of child modules (may be empty for terminal modules).\n",
    "    \"\"\"\n",
    "    def __init__(self, children: List[\"Module\"]=[]) -> None:\n",
    "        # set child modules\n",
    "        self.children = children\n",
    "        \n",
    "        # infer number of inputs from children (and their numbers of outputs)\n",
    "        child_num_outputs = [child.n_out for child in self.children]\n",
    "        child_cum_outputs = np.cumsum(child_num_outputs)\n",
    "        \n",
    "        self.n_in = sum(child_num_outputs, 0)\n",
    "\n",
    "        # compute conversion from input ids corresponding child and output id (Saves computation at run-time)\n",
    "        self.input_to_output_id_dict = {}\n",
    "        \n",
    "        for input_id in range(self.n_in):\n",
    "            # get child module for corresponding input\n",
    "            child_id = np.sum(child_cum_outputs <= input_id, axis=0).tolist()\n",
    "            # get output id of child module for corresponding input\n",
    "            output_id = input_id-(child_cum_outputs[child_id]-child_num_outputs[child_id])\n",
    "            \n",
    "            self.input_to_output_id_dict[input_id] = (child_id, output_id)\n",
    "\n",
    "    @abstractproperty\n",
    "    def n_out(self) -> int:\n",
    "        \"\"\"Specifies the number of outputs, i.e. (implicit of explicit) output nodes.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def input_to_output_id(self, input_id) -> Tuple[int, int]:\n",
    "        \"\"\"Helper method to convert an input id to a corresponding child and child output id.\"\"\"\n",
    "        return self.input_to_output_id_dict[input_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Sampling should return an array in a form that could immediately be used to infer likelihoods of that same module. That entails, that the RVs are in ascending order and that no RVs are skipped (e.g. sampling over a scope of `[0,2,5]` should return an array of size `(n,6)` to accommodate for all scope RVs at their respective indices while leaving all other entries as NaNs; `n` is the number of samples).\n",
    "\n",
    "API:\n",
    "- `sample(module)` should return return a single sample (size `(1,m)`)\n",
    "- `sample(module, n)` should return return `n` samples (size `(n,m)`)\n",
    "- `sample(module, array)` should fill the specified array in-place and return it as well (array must be of appropriate size). This also allows to specify incomplete data that is not replaced during sampled and whose likelihoods are taken into account while sampling.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When sampling for multiple instances, we need to keep track which modules are supposed to sample which instances.\n",
    "\n",
    "Example:\n",
    "![sampling_context_example](img/sampling_context_example.drawio.svg)\n",
    "In this case, the sum node samples a branch (B or C) for each instance, so A and B are sampling into the same data array, but get different instances to sample.\n",
    "\n",
    "In the multi-output case (i.e. modules, see below) one additionally needs to specify which outputs are supposed to be sampled.\n",
    "\n",
    "Example:\n",
    "![sampling_context_example](img/sampling_context_example_2.drawio.svg)\n",
    "Here, we essentially have the same graph as above, but `sample(...)` is called on the same child module for all instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SamplingContext` class to track which instances to sample and which module outputs to sample from. Keeps a list of instance ids to fill with samples and corresponding output ids to sample from for these instances for a given module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sampling_context](uml/sampling_context.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplingContext:\n",
    "    \"\"\"Keeps track of instance ids to sample and which output ids to sample from (relevant for modules with multiple outputs).\n",
    "    \n",
    "    Args:\n",
    "        instance_ids: list of ints representing the instances to sample.\n",
    "        output_ids: list of lists of ints representing the output ids for the corresponding instances to sample from (relevant for multi-output module).\n",
    "                    As a shorthand, '[]' implies to sample from all outputs for a given instance id.\n",
    "    \"\"\"\n",
    "    def __init__(self, instance_ids: List[int], output_ids: Optional[List[List[int]]]=None) -> None:\n",
    "\n",
    "        if output_ids is None:\n",
    "            # assume sampling over all outputs (i.e. [])\n",
    "            output_ids = [[] for _ in instance_ids]\n",
    "        \n",
    "        if (len(output_ids) != len(instance_ids)):\n",
    "            raise ValueError(f\"Number of specified instance ids {len(instance_ids)} does not match specified number of output ids {len(output_ids)}.\")\n",
    "\n",
    "        self.instance_ids = instance_ids\n",
    "        self.output_ids = output_ids\n",
    "\n",
    "    def select(self, ids: List[int]) -> \"SamplingConctext\":\n",
    "        \"\"\"Selects a subset of the instance ids and their corresponding output ids in a new sampling context object.\"\"\"\n",
    "        selection = [pair for i, pair in enumerate(zip(self.instance_ids, self.output_ids)) if i in ids]\n",
    "        return SamplingContext(*tuple(list(item) for item in zip(*selection)))\n",
    "\n",
    "    def append(self, instance_ids: List[int], output_ids: List[List[int]]) -> None:\n",
    "        \"\"\"Adds new instance ids and corresponding output ids to the sampling context.\"\"\"\n",
    "        if(len(instance_ids) != len(output_ids)):\n",
    "            raise ValueError(f\"Number of specified instance ids {len(instance_ids)} does not match specified number of output ids {len(output_ids)}.\")\n",
    "        \n",
    "        self.instance_ids += instance_ids\n",
    "        self.output_ids += output_ids\n",
    "\n",
    "    def remove(self, ids: List[int]) -> None:\n",
    "        \"\"\"Removes a subset of the instance ids and their corresponding output ids from the sampling context object.\"\"\"\n",
    "        selection = [pair for i, pair in enumerate(zip(self.instance_ids, self.output_ids)) if i not in ids]\n",
    "        self.instance_ids, self.output_ids = tuple(list(item) for item in zip(*selection))\n",
    "    \n",
    "    def is_valid(self, data: Optional[np.ndarray]=None, module: Optional[Module]=None) -> bool:\n",
    "        \"\"\"Returns a boolean whether or not the sampling context object is valid. Additionally, a data array and a module may be specified).\"\"\"\n",
    "        \n",
    "        # check if there are any duplicate instance ids\n",
    "        if len(set(self.instance_ids)) != len(self.instance_ids):\n",
    "            return False\n",
    "        \n",
    "        # validate individual instance ids\n",
    "        for instance_id in self.instance_ids:\n",
    "            # check if all instance ids are greater or equal to 0\n",
    "            if instance_id < 0:\n",
    "                return False\n",
    "            # if a data array is specified, then also check if the instance ids are valid for it\n",
    "            if (data is not None) and (instance_id >= data.shape[0]):\n",
    "                return False\n",
    "        \n",
    "        # check if number of output ids matches number of input ids\n",
    "        if len(self.instance_ids) != len(self.output_ids):\n",
    "            return False\n",
    "        \n",
    "        # check if any output id lists contain duplicates\n",
    "        for out_ids in self.output_ids:\n",
    "            if len(set(out_ids)) != len(out_ids):\n",
    "                return False\n",
    "        \n",
    "        # validate individual output ids\n",
    "        for out_ids in self.output_ids:\n",
    "            for out_id in out_ids:\n",
    "                # Check if all output ids are greater of equal to 0\n",
    "                if out_id < 0:\n",
    "                    return False\n",
    "                # if a module is specified, then also check if the output ids are valid indices for it\n",
    "                if (module is not None) and (out_id >= module.n_out):\n",
    "                    return False\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 6] [[1], [], [], [4]]\n"
     ]
    }
   ],
   "source": [
    "sc = SamplingContext([0,2,4,6],[[1],[],[],[4]])\n",
    "print(sc.instance_ids, sc.output_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 6] [[], [4]]\n"
     ]
    }
   ],
   "source": [
    "# select certain instance ids (e.g. to pass to a child module for sampling)\n",
    "sc2 = sc.select([1,3])\n",
    "print(sc2.instance_ids, sc2.output_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 4] [[1], []]\n"
     ]
    }
   ],
   "source": [
    "# remove certain instance ids\n",
    "sc.remove([1,3])\n",
    "print(sc.instance_ids, sc.output_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dispatch sampling for all modules without specified data tensor (see API-notes above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dispatch(Module, ll_cache=dict)\n",
    "def sample(module: Module, ll_cache: dict={}) -> np.ndarray:\n",
    "    \"\"\"Dispatches sampling a single instance from the module.\"\"\"\n",
    "    return sample(module, 1, ll_cache=ll_cache)\n",
    "\n",
    "@dispatch(Module, int, ll_cache=dict, sampling_context=SamplingContext)\n",
    "def sample(module: Module, n_samples: int, ll_cache: dict={}, sampling_context: SamplingContext=None) -> np.ndarray:\n",
    "    \"\"\"Creates an array for n samples and dispatches sampling to fill the array\"\"\"\n",
    "    if(not sampling_context):\n",
    "        # create sampling context (assume all instances are sampled from)\n",
    "        sampling_context = SamplingContext(list(range(n_samples)), [[] for _ in range(n_samples)])\n",
    "\n",
    "    # get module scope and largest random variable id\n",
    "    module_scope = scope(module).squeeze().tolist()\n",
    "    max_rv = max(module_scope.variables)\n",
    "\n",
    "    # create appropriate data tensor to fill\n",
    "    data = np.full((n_samples,max_rv+1), float(\"nan\"))\n",
    "\n",
    "    # sample and fill data tensor\n",
    "    return sample(module, data, ll_cache=ll_cache, sampling_context=sampling_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Log-)Likelihood\n",
    "\n",
    "In general, we would like to offer both `likelihood` and `loglikelihood` routines. For better comprehension, we only implement `likelihood` in this module. Note, that we can easily implement one from the other by simply calling `log(..)` or `exp(..)`, respectively.\n",
    "\n",
    "API:\n",
    "- `likelihood(module, data)`: should return an array of the size `(n,1)`. `np.nan` values are marginalized over (not necessarily demonstrated here).\n",
    "- `likelihood(module, data, dict)`: specifies a dictionary (empty, partially or fully filled). For each module check if it is in the dictionary keys and reuses the stored value or computes it and updates the cache afterwards. The cache can then be reused for later calls (with the same data) or e.g. for the `sampling` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memoization Decorator\n",
    "\n",
    "E.g. can wrap the `likelihood` function and automatically check the cache and re-use or fill it.\n",
    "\n",
    "**NOTE**: the `memoize` decorate must come after the `dispatch` decorator (before in the decorator evaluation logic), so that the memoized version of the function is being dispatched!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![memoize](uml/memoize.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "def memoize(f):\n",
    "    \"\"\"Wraps a function to automatically check against a cache ('cache' keyword argument) using the first argument as the key.\n",
    "    If present, the cached value is returned, otherwise it is computed using the wrapped function stored in the cache.\n",
    "    \"\"\"\n",
    "    @wraps(f)\n",
    "    def memoized_f(*args, **kwargs):\n",
    "        # get cache (initialize if non specified)\n",
    "        kwargs.setdefault(\"cache\", {})\n",
    "        cache = kwargs[\"cache\"]\n",
    "\n",
    "        # args contains key variable to be used for cache (assumes key variable is first positional argument to f)\n",
    "        if len(args) > 0:\n",
    "            key = args[0]\n",
    "        # key variable must be part of kwargs\n",
    "        else:\n",
    "            raise ValueError(\"No argument to cache against\")\n",
    "\n",
    "        if key not in cache:\n",
    "            # compute result and update cache\n",
    "            cache[key] = f(*args, **kwargs)\n",
    "\n",
    "        return cache[key]\n",
    "    return memoized_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Modules\n",
    "\n",
    "Basic `LeafNode`,`SumNode` and `ProductNode` classes.\n",
    "\n",
    "Every class dispatches `likelihood(...)`, `scope(...)` and `sample(...)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nodes](uml/nodes.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract Node Module\n",
    "\n",
    "Fixed outputs size (i.e. `__len__`) of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(Module, ABC):\n",
    "    \"\"\"Represents basic nodes, i.e. the smallest building block for non-optimized networks.\"\"\"\n",
    "    @property\n",
    "    def n_out(self) -> int:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaf Nodes\n",
    "\n",
    "**Note**: `LeafNode` should actually be an abstract base class for leaf nodes to inherit from. In this example we simply implement `LeafNode` as a one-dimensional Gaussian distribution for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeafNode(Node):\n",
    "    \"\"\"Basic univariate leaf node. Here, implemented as a Gaussian.\n",
    "\n",
    "    Args:\n",
    "        scope: scope of the distribution.\n",
    "        mean: mean of the distribution.\n",
    "        std: standard deviation of the distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, scope: Scope, mean=0.0, std=1.0) -> None:\n",
    "        \n",
    "        if(len(scope) != 1):\n",
    "            raise ValueError(\"Scope to large for univariate leaf node.\")\n",
    "        \n",
    "        super(LeafNode, self).__init__()\n",
    "        \n",
    "        self.scope = scope\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.dist = norm(loc=mean, scale=std)\n",
    "\n",
    "@dispatch(LeafNode, cache=dict)\n",
    "@memoize\n",
    "def scope(module: LeafNode, cache: dict={}) -> ScopeArray:\n",
    "    \"\"\"Simply returns the scope of the module, since it's a leaf node.\"\"\"\n",
    "    return ScopeArray([[module.scope]])\n",
    "\n",
    "@dispatch(LeafNode, np.ndarray, cache=dict)\n",
    "@memoize\n",
    "def likelihood(module: LeafNode, data: np.ndarray, cache: dict={}) -> np.ndarray:\n",
    "    \"\"\"Returns the likelihood for each data instance. Marginalizes (likelihood of 1) for NaN values.\"\"\"\n",
    "    likelihoods = np.ones((data.shape[0], len(module.scope)))\n",
    "    inputs = data[:, list(module.scope.variables)]\n",
    "\n",
    "    marg_ids = np.isnan(inputs).sum(axis=1) == len(module.scope)\n",
    "    likelihoods[~marg_ids, :] = module.dist.pdf(inputs[~marg_ids, :])\n",
    "\n",
    "    return likelihoods\n",
    "\n",
    "@dispatch(LeafNode, np.ndarray, ll_cache=dict, sampling_context=SamplingContext)\n",
    "def sample(module: LeafNode, data: np.ndarray, ll_cache: dict={}, sampling_context: Optional[SamplingContext]=None) -> np.ndarray:\n",
    "    \"\"\"Samples and fills the indices specified in the sampling context.\"\"\"\n",
    "\n",
    "    if sampling_context:\n",
    "        if not sampling_context.is_valid(data, module):\n",
    "            # invalid sampling context\n",
    "            raise ValueError(f\"Specified sampling context is invalid for specified data array and module.\")\n",
    "    else:\n",
    "        # create sampling context (assume all instances and all output nodes are to be used)\n",
    "        sampling_context = SamplingContext(list(range(data.shape[0])), [[] for _ in range(data.shape[0])]) \n",
    "\n",
    "    scope_vars = list(module.scope.variables)\n",
    "\n",
    "    # sample ids (nan entries)\n",
    "    sample_ids = (np.isnan(data[:, scope_vars]).sum(axis=1) == len(scope_vars))\n",
    "    \n",
    "    # get mask for sampling context\n",
    "    instance_id_mask = np.zeros(data.shape[0]).astype(bool)\n",
    "    instance_id_mask[sampling_context.instance_ids] = True\n",
    "\n",
    "    # filter instance ids according to sampling context\n",
    "    sample_ids &= instance_id_mask\n",
    "\n",
    "    # sample\n",
    "    data[sample_ids, list(module.scope.variables)] = np.random.normal(module.mean, module.std, sample_ids.sum())\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductNode(Node):\n",
    "    \"\"\"Simple SPN product node. Child nodes (explicit or implicit) are assumed to have pairwise disjoint scopes.\n",
    "    \n",
    "    Args:\n",
    "        children: list of child modules.\n",
    "    \"\"\"\n",
    "    def __init__(self, children: List[Module]) -> None:\n",
    "        \n",
    "        if(len(children) == 0):\n",
    "            raise ValueError(f\"List of child modules for ProductNode is empty.\")\n",
    "\n",
    "        super(ProductNode, self).__init__(children)\n",
    "\n",
    "@dispatch(ProductNode, cache=dict)\n",
    "@memoize\n",
    "def scope(module: ProductNode, cache: dict={}) -> ScopeArray:\n",
    "    \"\"\"Returns merged child scopes.\"\"\" \n",
    "    scopes = np.concatenate([scope(child, cache=cache) for child in module.children], axis=1)\n",
    "    return scopes.sum(keepdims=True)\n",
    "\n",
    "@dispatch(ProductNode, np.ndarray, cache=dict)\n",
    "@memoize\n",
    "def likelihood(module: ProductNode, data: np.ndarray, cache: dict={}) -> np.ndarray:\n",
    "    \"\"\"Returns the product of the child likelihoods.\"\"\" \n",
    "    inputs = np.concatenate([likelihood(child, data, cache=cache) for child in module.children], axis=1)\n",
    "    return np.prod(inputs, axis=1, keepdims=True)\n",
    "\n",
    "@dispatch(ProductNode, np.ndarray, ll_cache=dict, sampling_context=SamplingContext)\n",
    "def sample(module: ProductNode, data: np.ndarray, ll_cache: dict={}, sampling_context=None) -> np.ndarray:\n",
    "    \"\"\"Samples from all branches (i.e. nodes/module outputs).\"\"\" \n",
    "    \n",
    "    if sampling_context:\n",
    "        if not sampling_context.is_valid(data, module):\n",
    "            # invalid sampling context\n",
    "            raise ValueError(f\"Specified sampling context is invalid for specified data array and module.\")\n",
    "    else:\n",
    "        # create sampling context (assume all instances and all output nodes are to be used)\n",
    "        sampling_context = SamplingContext(list(range(data.shape[0])), [[] for _ in range(data.shape[0])])\n",
    "\n",
    "    # sample from all branches (i.e. inputs)\n",
    "    for child in module.children:\n",
    "        # create sampling context for child (same instance ids but different output ids now for new module)\n",
    "        child_sampling_context = SamplingContext(sampling_context.instance_ids, [[] for _ in range(len(sampling_context.instance_ids))])\n",
    "        # sample from child\n",
    "        sample(child, data, ll_cache=ll_cache, sampling_context=child_sampling_context)\n",
    "        \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumNode(Node):\n",
    "    \"\"\"Simple SPN sum node. merged child scopes. All child nodes (explicit or implicit) are assumed to have the same scopes.\n",
    "    \n",
    "    Args:\n",
    "        children: list of child modules.\n",
    "    \"\"\" \n",
    "    def __init__(self, children) -> None:\n",
    "        \n",
    "        if(len(children) == 0):\n",
    "            raise ValueError(f\"List of child modules for SumNode is empty.\")\n",
    "        \n",
    "        super(SumNode, self).__init__(children)\n",
    "        \n",
    "        self.weights = np.random.rand(self.n_in)\n",
    "        self.weights /= self.weights.sum()\n",
    "\n",
    "@dispatch(SumNode, cache=dict)\n",
    "def scope(module: SumNode, cache: dict={}) -> ScopeArray:\n",
    "    \"\"\"Returns merged child scopes.\"\"\"\n",
    "    scopes = np.concatenate([scope(child, cache=cache) for child in module.children])\n",
    "    return scopes.sum(keepdims=True)\n",
    "\n",
    "@dispatch(SumNode, np.ndarray, cache=dict)\n",
    "@memoize\n",
    "def likelihood(module: SumNode, data: np.ndarray, cache: dict={}) -> np.ndarray:\n",
    "    \"\"\"Returns the weighted sum of the child likelihoods.\"\"\"\n",
    "    inputs = np.concatenate([likelihood(child, data, cache=cache) for child in module.children], axis=1)\n",
    "    return (module.weights*inputs).sum(axis=1, keepdims=True)\n",
    "\n",
    "@dispatch(SumNode, np.ndarray, ll_cache=dict, sampling_context=SamplingContext)\n",
    "def sample(module: SumNode, data: np.ndarray, ll_cache: dict={}, sampling_context=None) -> np.ndarray:\n",
    "    \"\"\"Samples a branch for each instance (taking likelihoods into account).\"\"\"\n",
    "    \n",
    "    if sampling_context:\n",
    "        if not sampling_context.is_valid(data, module):\n",
    "            # invalid sampling context\n",
    "            raise ValueError(f\"Specified sampling context is invalid for specified data array and module.\")\n",
    "    else:\n",
    "        # create sampling context (assume all instances and all output nodes are to be used)\n",
    "        sampling_context = SamplingContext(list(range(data.shape[0])), [[] for _ in range(data.shape[0])])\n",
    "\n",
    "    # get likelihoods for children (using cache; only use relevant instances to save computation)\n",
    "    child_likelihoods = np.concatenate([likelihood(child, data[sampling_context.instance_ids], cache=ll_cache) for child in module.children], axis=1)\n",
    "    \n",
    "    # sample branch for each instance id\n",
    "    choices = []\n",
    "    sampling_probs = child_likelihoods * module.weights\n",
    "    \n",
    "    for probs in sampling_probs:\n",
    "        # TODO: normalize? -> yes have to sum to 1\n",
    "        probs_norm = probs * (1 / np.sum(probs))\n",
    "        choices.append(np.random.choice(list(range(probs.shape[0])), p=probs_norm))\n",
    "\n",
    "    choices = np.array(choices)\n",
    "\n",
    "    # get number of outputs per child module\n",
    "    child_num_outputs = np.array([child.n_out for child in module.children])\n",
    "    child_cum_outputs = np.cumsum(child_num_outputs)\n",
    "\n",
    "    # for each unique sampled branch\n",
    "    for branch_id in np.unique(choices):\n",
    "        # group instances by sampled branch\n",
    "        child_sample_ids = np.where(choices == branch_id)[0]\n",
    "\n",
    "        # get corresponding child and output id for sampled branch\n",
    "        child_id, output_id = module.input_to_output_id(branch_id)\n",
    "    \n",
    "        # sample from child\n",
    "        sample(module.children[child_id], data, ll_cache=ll_cache, sampling_context=SamplingContext(child_sample_ids, [[output_id] for _ in range(len(child_sample_ids))]))\n",
    "        \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: [[1.03918384]], Sample likelihood: [[0.23249419]]\n"
     ]
    }
   ],
   "source": [
    "l1 = LeafNode(Scope(SPN(), [0])) # leaf node with (scope 0)\n",
    "l2 = LeafNode(Scope(SPN(), [0])) # leaf node with (scope 1)\n",
    "s = SumNode(children=[l1,l2]) # sum node over both product nodes (scope 0,1)\n",
    "\n",
    "samples = sample(s)\n",
    "print(f\"Sample: {samples}, Sample likelihood: {likelihood(s, samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.406048    1.94702653]\n",
      " [ 0.67333241 -0.29821458]\n",
      " [-1.81747117 -0.23109304]]\n",
      "Sample: [[ 0.406048    1.94702653]\n",
      " [ 0.67333241 -0.29821458]\n",
      " [-1.81747117 -0.23109304]], Sample likelihood: [[0.02202074]\n",
      " [0.12135547]\n",
      " [0.02971273]]\n",
      "Scope: [[Scope({0, 1})]]\n"
     ]
    }
   ],
   "source": [
    "l1 = LeafNode(Scope(SPN(), [0])) # leaf node with (scope 0)\n",
    "l2 = LeafNode(Scope(SPN(), [1])) # leaf node with (scope 1)\n",
    "p1 = ProductNode([l1,l2]) # product node over both leaf nodes (scope 0,1)\n",
    "p2 = ProductNode([l1,l2]) # product node over both leaf nodes (scope 0,1)\n",
    "s = SumNode(children=[p1,p2]) # sum node over both product nodes (scope 0,1)\n",
    "\n",
    "data = np.random.randn(3,2)\n",
    "#data[0][0] = np.nan\n",
    "print(data)\n",
    "\n",
    "samples = sample(s, data)\n",
    "print(f\"Sample: {samples}, Sample likelihood: {likelihood(s, samples)}\")\n",
    "print(f\"Scope: {scope(s)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood (w/o cache):\t\t[[0.15453114]]\n",
      "Likelihood (store cache):\t[[0.15453114]], matches stored value: True\n",
      "Likelihood (modified cache):\t[[1.]]\n"
     ]
    }
   ],
   "source": [
    "data = np.random.randn(1,2)\n",
    "\n",
    "# call without passing cache\n",
    "l = likelihood(s, data)\n",
    "print(f\"Likelihood (w/o cache):\\t\\t{l}\")\n",
    "\n",
    "# use cache this time\n",
    "d = {}\n",
    "l = likelihood(s, data, cache=d)\n",
    "print(f\"Likelihood (store cache):\\t{l}, matches stored value: {all(l == d[s])}\")\n",
    "\n",
    "# set cache to check if stored value is used\n",
    "d[s] = np.ones((1,1))\n",
    "l = likelihood(s, data, cache=d)\n",
    "print(f\"Likelihood (modified cache):\\t{l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without any specifications: (1, 1)\n",
      "Specifying number of samples: (5, 1)\n",
      "Passing data tensor: True\n"
     ]
    }
   ],
   "source": [
    "l = LeafNode(Scope(SPN(), [0]))\n",
    "\n",
    "d = sample(l)\n",
    "print(\"Without any specifications:\", d.shape)\n",
    "\n",
    "d = sample(l, 5)\n",
    "print(\"Specifying number of samples:\", d.shape)\n",
    "\n",
    "d = np.full((3,1), np.nan)\n",
    "d_ = sample(l, d)\n",
    "print(\"Passing data tensor:\", all(d == d_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested Modules\n",
    "\n",
    "We'd like to build more complex modules from basic nodes. These could then again be combined to create even more intricate modules. For that we need to be able to nest modules.\n",
    "\n",
    "A network without any open non-terminal nodes/modules, can straight-forwardly be nested.\n",
    "\n",
    "However, non-terminal modules need child modules to be specified at creation. In nested modules this would require internal non-terminal modules to reference the same child modules as the enclosing module. This would be extremely messy.\n",
    "\n",
    "Instead, one could use placeholder modules that can stand-in for the actual children. The enclosing module can the set the cache for `scope`,`likelihood` calls to these modules or redirect `sample`-calls to the actual child modules. This also allows to divide up inputs from child modules for the internal/nested modules and change their order (in contrast to direct parent-child relationships).\n",
    "\n",
    "![nesting_module](uml/nesting_module.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NestingModule(Module, ABC):\n",
    "    \"\"\"Convenient module class for nesting non-terminal modules.\n",
    "    \n",
    "    Args:\n",
    "        childen: list of child modules.\n",
    "    \"\"\"\n",
    "    def __init__(self, children: List[Module]=[]) -> None:\n",
    "        super(NestingModule, self).__init__(children)\n",
    "        self.placeholders = []\n",
    "\n",
    "    def create_placeholder(self, input_ids: List[int]) -> \"Placeholder\":\n",
    "        \"\"\"Creates a placholder module that can be used for internal non-terminal modules.\n",
    "        \n",
    "        Also registers the placeholder internally.\n",
    "        \"\"\"\n",
    "        # create and register placeholder\n",
    "        ph = self.Placeholder(self, input_ids)\n",
    "        self.placeholders.append(ph)\n",
    "\n",
    "        return ph\n",
    "    \n",
    "    def set_placeholders(self, cache, inputs) -> None:\n",
    "        \"\"\"Fills the cache for all registered placeholder modules given specified input values.\"\"\"\n",
    "        for ph in self.placeholders:\n",
    "            # fill placeholder cache with specified input values\n",
    "            cache[ph] = inputs[:,ph.input_ids]\n",
    "\n",
    "    class Placeholder(Module):\n",
    "        \"\"\"Placeholder module as an intermediary module between nested non-terminal modules and actual child modules.\"\"\"\n",
    "        def __init__(self, owner: Module, input_ids: List[int]) -> None:\n",
    "            self.owner = owner\n",
    "            self.input_ids = input_ids\n",
    "            \n",
    "            # compute conversion from input ids corresponding child and output id (Saves computation at run-time)\n",
    "            self.input_to_output_id_dict = {}\n",
    "            \n",
    "            #print(\"input ids:\", self.input_ids)\n",
    "\n",
    "            for input_id in range(len(input_ids)):\n",
    "                # convert placeholder input id to actual input id\n",
    "                #print(input_id)\n",
    "                input_id_actual = self.input_ids[input_id]\n",
    "\n",
    "                # set corresponding child and output id via owner\n",
    "                self.input_to_output_id_dict[input_id] = self.owner.input_to_output_id(input_id_actual)\n",
    "\n",
    "        @property\n",
    "        def n_out(self) -> int:\n",
    "            return len(self.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dispatch(NestingModule.Placeholder, np.ndarray, cache=dict)\n",
    "@memoize\n",
    "def likelihood(module: NestingModule.Placeholder, data: np.ndarray, cache: dict={}):\n",
    "    \"\"\"Gets called if values for placeholder module are not in the cache. In that case raise an error.\"\"\"\n",
    "    raise LookupError(\"Likelihood values for placeholder module not found in cache. Check if these are correctly set by the nesting module.\")\n",
    "\n",
    "@dispatch(NestingModule.Placeholder, cache=dict)\n",
    "@memoize\n",
    "def scope(module: NestingModule.Placeholder, cache: dict={}):\n",
    "    \"\"\"Gets called if values for placeholder module are not in the cache. In that case raise an error.\"\"\"\n",
    "    raise LookupError(\"Scope values for placeholder module not found in cache. Check if these are correctly set by the nesting module.\")\n",
    "\n",
    "@dispatch(NestingModule.Placeholder, np.ndarray, ll_cache=dict, sampling_context=SamplingContext)\n",
    "def sample(module: NestingModule.Placeholder, data: np.ndarray, ll_cache: dict={}, sampling_context=None):\n",
    "    \"\"\"Redirects sampling calls to sample actual child modules\"\"\"\n",
    "    \n",
    "    if sampling_context:\n",
    "        if not sampling_context.is_valid(data, module):\n",
    "            # invalid sampling context\n",
    "            raise ValueError(f\"Specified sampling context is invalid for specified data array and module.\")\n",
    "    else:\n",
    "        if(module.n_out != 1):\n",
    "            raise ValueError(\"No sampling context specified. It is unclear which output to sample from.\")\n",
    "        else:\n",
    "            sampling_context = SamplingContext(list(range(len(data.shape[0]))), [[] for _ in len(data.shape[0])])\n",
    "\n",
    "    sampling_context_per_child = {}\n",
    "\n",
    "    # TODO: could potentially be done more efficiently via grouping\n",
    "    for instance_id, instance_output_ids in zip(sampling_context.instance_ids, sampling_context.output_ids):\n",
    "       \n",
    "        output_per_child = {}\n",
    "        \n",
    "        # iterate over actual child and output ids\n",
    "        if instance_output_ids == []:\n",
    "            # all children\n",
    "        \n",
    "            for _, ids in module.input_to_output_id_dict.items():\n",
    "                output_per_child[ids[0]] = [ids[1]]\n",
    "        else:\n",
    "            for child_id, output_id in [module.input_to_output_id(output_id) for output_id in instance_output_ids]:\n",
    "\n",
    "                # sort output ids per child id\n",
    "                if(child_id in output_per_child):\n",
    "                    output_per_child[child_id].append(output_id)\n",
    "                else:\n",
    "                    output_per_child[child_id] = [output_id]\n",
    "        \n",
    "        # append (or create) sampling contexts\n",
    "        for child_id, output_ids in output_per_child.items():\n",
    "            if(child_id) in sampling_context_per_child:\n",
    "                sampling_context_per_child[child_id].instance_ids.append(instance_id)\n",
    "                sampling_context_per_child[child_id].output_ids.append(output_ids)\n",
    "            else:\n",
    "                sampling_context_per_child[child_id] = SamplingContext([instance_id], [output_ids])\n",
    "   \n",
    "    # sample from children\n",
    "    for child_id, child_sampling_context in sampling_context_per_child.items():\n",
    "        sample(module.owner.children[child_id], data, ll_cache=ll_cache, sampling_context=child_sampling_context)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Modules\n",
    "\n",
    "Basic `LeafLayer`,`SumLayer` and `ProducLayer` classes.\n",
    "\n",
    "![layers](uml/layers.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leaf Layer\n",
    "\n",
    "Note: `LeafLayer` only contains terminal modules and therefore has no need for placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeafLayer(Module):\n",
    "    \"\"\"Layer of multiple leaf nodes over the same scope.\n",
    "    \n",
    "    Args:\n",
    "        scope: scope of all leaf nodes.\n",
    "        n_out: number of leaf nodes.\n",
    "    \"\"\"\n",
    "    def __init__(self, scope: Scope, n_out) -> None:\n",
    "        \n",
    "        super(LeafLayer, self).__init__()\n",
    "        \n",
    "        self.nodes = []\n",
    "        self.scope = scope\n",
    "\n",
    "        # create leaf nodes\n",
    "        for _ in range(n_out):\n",
    "            self.nodes.append(LeafNode(scope=scope))\n",
    "    \n",
    "    @property\n",
    "    def n_out(self) -> int:\n",
    "        return len(self.nodes)\n",
    "\n",
    "@dispatch(LeafLayer, cache=dict)\n",
    "@memoize\n",
    "def scope(module: LeafLayer, cache: dict={}) -> ScopeArray:\n",
    "    \"\"\"Concatenates the scopes of all leaf nodes.\"\"\"\n",
    "    return np.concatenate([scope(node, cache=cache) for node in module.nodes], axis=1)\n",
    "\n",
    "@dispatch(LeafLayer, np.ndarray, cache=dict)\n",
    "@memoize\n",
    "def likelihood(module: LeafLayer, data: np.ndarray, cache: dict={}) -> np.ndarray:\n",
    "    \"\"\"Concatenates the likelihoods for all leaf nodes.\"\"\"\n",
    "    return np.concatenate([likelihood(node, data, cache=cache) for node in module.nodes], axis=1)\n",
    "\n",
    "@dispatch(LeafLayer, np.ndarray, ll_cache=dict, sampling_context=SamplingContext)\n",
    "def sample(module: LeafLayer, data: np.ndarray, ll_cache: dict={}, sampling_context=None) -> np.ndarray:\n",
    "    \"\"\"Samples leaf nodes accoding to sampling context.\"\"\"\n",
    "    #print(sampling_context.instance_ids)\n",
    "\n",
    "    if sampling_context:\n",
    "        if not sampling_context.is_valid(data, module):\n",
    "            # invalid sampling context\n",
    "            raise ValueError(f\"Specified sampling context is invalid for specified data array and module.\")\n",
    "    else:\n",
    "        raise ValueError(\"No sampling context specified. It is unclear which output to sample from.\")\n",
    "        # create sampling context (assume all instances and all output nodes are to be used)\n",
    "        #sampling_context = SamplingContext(list(range(data.shape[0])), [[] for _ in range(data.shape[0])])\n",
    "    \n",
    "    for node_ids in np.unique(sampling_context.output_ids, axis=0):\n",
    "        if(len(node_ids) != 1):\n",
    "            raise ValueError(\"Too many output ids specified for outputs over same scope.\")\n",
    "        \n",
    "        node_id = node_ids[0]\n",
    "        node_instance_ids = np.where(sampling_context.output_ids == node_ids)[0]\n",
    "        node_instance_ids_test = sampling_context.instance_ids\n",
    "        \n",
    "        sample(module.nodes[node_id], data, ll_cache=ll_cache, sampling_context=SamplingContext(node_instance_ids_test, [[] for i in node_instance_ids]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood: (2, 3)\n",
      "Scope: [[Scope({0}) Scope({0}) Scope({0})]]\n"
     ]
    }
   ],
   "source": [
    "layer = LeafLayer(Scope(SPN(), [0]), 3)\n",
    "\n",
    "data = np.random.randn(2,1)\n",
    "print(f\"Likelihood: {likelihood(layer, data).shape}\")\n",
    "\n",
    "print(f\"Scope: {scope(layer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08297686]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = SumNode(children=[layer])\n",
    "sample(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum Layer\n",
    "\n",
    "Because this layer now uses non-terminal modules/nodes, we use `NestingModule` as a blueprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumLayer(NestingModule):\n",
    "    \"\"\"Layer of multiple sum nodes over the same child modules.\n",
    "    \n",
    "    Args:\n",
    "        n_out: number of sum nodes.\n",
    "        children: list of child modules.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_out, children) -> None:\n",
    "        super(SumLayer, self).__init__(children=children)\n",
    "        \n",
    "        self.nodes = []\n",
    "\n",
    "        # all nodes share the same input link here\n",
    "        ph = self.create_placeholder(list(range(self.n_in)))\n",
    "\n",
    "        for _ in range(n_out):\n",
    "            self.nodes.append(SumNode([ph]))\n",
    "\n",
    "    @property\n",
    "    def n_out(self) -> int:\n",
    "        return len(self.nodes)\n",
    "\n",
    "@dispatch(SumLayer, cache=dict)\n",
    "@memoize\n",
    "def scope(module: SumLayer, cache: dict={}) -> ScopeArray:\n",
    "    \"\"\"Concatenates the scopes of all sum nodes.\"\"\"\n",
    "    input_scopes = np.concatenate([scope(child, cache=cache) for child in module.children], axis=1)\n",
    "\n",
    "    # set placeholders\n",
    "    module.set_placeholders(cache, input_scopes)\n",
    "    \n",
    "    # compute output scopes\n",
    "    output_scopes = np.concatenate([scope(node, cache=cache) for node in module.nodes], axis=1)\n",
    "    \n",
    "    return output_scopes\n",
    "\n",
    "@dispatch(SumLayer, np.ndarray, cache=dict)\n",
    "@memoize\n",
    "def likelihood(module: SumLayer, data: np.ndarray, cache: dict={}) -> np.ndarray:\n",
    "    \"\"\"Concatenates the likelihoods for all sum nodes.\"\"\"\n",
    "    input_likelihoods = np.concatenate([likelihood(child, data, cache=cache) for child in module.children], axis=1)\n",
    "    \n",
    "    # set placeholders\n",
    "    module.set_placeholders(cache, input_likelihoods)\n",
    "    \n",
    "    # compute output likelihoods\n",
    "    output_scopes = np.concatenate([likelihood(node, data, cache=cache) for node in module.nodes], axis=1)\n",
    "    \n",
    "    return output_scopes\n",
    "\n",
    "@dispatch(SumLayer, np.ndarray, ll_cache=dict, sampling_context=SamplingContext)\n",
    "def sample(module: SumLayer, data: np.ndarray, ll_cache: dict={}, sampling_context=None) -> np.ndarray:\n",
    "    \"\"\"Samples leaf nodes accoding to sampling context.\"\"\"\n",
    "    \n",
    "    if sampling_context:\n",
    "        if not sampling_context.is_valid(data, module):\n",
    "            # invalid sampling context\n",
    "            raise ValueError(f\"Specified sampling context is invalid for specified data array and module.\")\n",
    "    else:\n",
    "        raise ValueError(\"No sampling context specified. It is unclear which output to sample from.\")\n",
    "\n",
    "    #print(\"sum layer sample\")\n",
    "    #print(sampling_context.instance_ids)\n",
    "    #print(sampling_context.output_ids)\n",
    "    # fix for [] case\n",
    "    for node_ids in np.unique(sampling_context.output_ids, axis=0):\n",
    "        if(len(node_ids) != 1):\n",
    "            raise ValueError(\"Too many output ids specified for outputs over same scope.\")\n",
    "\n",
    "        node_id = node_ids[0]\n",
    "        node_instance_ids = np.where(sampling_context.output_ids == node_ids)[0]\n",
    "\n",
    "        sample(module.nodes[node_id], data, ll_cache=ll_cache, sampling_context=SamplingContext(node_instance_ids, [[] for i in node_instance_ids]))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood: [[0.37929153 0.37929153 0.37929153]]\n",
      "Scope: [[Scope({0}) Scope({0}) Scope({0})]]\n"
     ]
    }
   ],
   "source": [
    "leaf_layer = LeafLayer(Scope(SPN(), [0]), 3)\n",
    "sum_layer = SumLayer(3, [leaf_layer])\n",
    "\n",
    "data = np.random.randn(1,1)\n",
    "print(f\"Likelihood: {likelihood(sum_layer, data)}\")\n",
    "\n",
    "print(f\"Scope: {scope(sum_layer)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample1: [-0.00090164]\n",
      "Sample2: [-0.00423772]\n"
     ]
    }
   ],
   "source": [
    "s = SumNode([leaf_layer])\n",
    "s1 = SumNode([LeafNode(Scope(SPN(), [0])), LeafNode(Scope(SPN(), [0])), LeafNode(Scope(SPN(), [0]))])\n",
    "\n",
    "#print(sample(s))\n",
    "#print(sample(s1))\n",
    "print(f\"Sample1: {np.mean(sample(s, 100000), axis=0)}\")\n",
    "print(f\"Sample2: {np.mean(sample(s1, 100000), axis=0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ProductLayer Class\n",
    "\n",
    "Number of `ProductNode`s is the number of combinations of elements from each input group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductLayer(NestingModule):\n",
    "    \"\"\"Layer of multiple product nodes over the same child modules.\n",
    "    \n",
    "    Creates a product node for each combination of inputs from the child modules.\n",
    "    E.g. for two modules with 2 (ids 0,1) and 3 (ids 2,3,4) outputs, respectively, one gets nodes with the following inputs:\n",
    "        [0,2]\n",
    "        [0,3]\n",
    "        [0,4]\n",
    "        [1,2]\n",
    "        [1,3]\n",
    "        [1,4]\n",
    "\n",
    "    Args:\n",
    "        children: list of child modules.\n",
    "    \"\"\"\n",
    "    def __init__(self, children) -> None:\n",
    "        super(ProductLayer, self).__init__(children)\n",
    "        \n",
    "        self.nodes = []\n",
    "        self.input_placeholders = []\n",
    "        \n",
    "        children_n_out = [child.n_out for child in self.children]\n",
    "        total_ids = list(range(sum(children_n_out)))\n",
    "        factorized_ids = []\n",
    "        \n",
    "        for n in children_n_out:\n",
    "            factorized_ids.append(total_ids[:n])\n",
    "            total_ids = total_ids[n:]\n",
    "\n",
    "        self.input_ids_per_node = list(itertools.product(*factorized_ids))\n",
    "        \n",
    "        # create product nodes\n",
    "        for ids in self.input_ids_per_node:\n",
    "            ph = self.create_placeholder(list(ids))\n",
    "            self.nodes.append(ProductNode(children=[ph]))\n",
    "    \n",
    "    @property\n",
    "    def n_out(self) -> int:\n",
    "        return len(self.nodes)\n",
    "\n",
    "@dispatch(ProductLayer, cache=dict)\n",
    "@memoize\n",
    "def scope(module: ProductLayer, cache: dict={}) -> ScopeArray:\n",
    "    \"\"\"Concatenates the scopes of all sum nodes.\"\"\"\n",
    "    input_scopes = np.concatenate([scope(child, cache=cache) for child in module.children], axis=1)\n",
    "\n",
    "    # set placeholders\n",
    "    module.set_placeholders(cache, input_scopes)\n",
    "    \n",
    "    # compute output scopes\n",
    "    output_scopes = np.concatenate([scope(node, cache=cache) for node in module.nodes], axis=1)\n",
    "    \n",
    "    return output_scopes\n",
    "\n",
    "@dispatch(ProductLayer, np.ndarray, cache=dict)\n",
    "@memoize\n",
    "def likelihood(module: ProductLayer, data: np.ndarray, cache: dict={}) -> np.ndarray:\n",
    "    \"\"\"Concatenates the likelihoods for all sum nodes.\"\"\"\n",
    "    input_likelihoods = np.concatenate([likelihood(child, data, cache=cache) for child in module.children], axis=1)\n",
    "    \n",
    "    # set placeholders\n",
    "    module.set_placeholders(cache, input_likelihoods)\n",
    "    \n",
    "    # compute output likelihoods\n",
    "    output_scopes = np.concatenate([likelihood(node, data, cache=cache) for node in module.nodes], axis=1)\n",
    "    \n",
    "    return output_scopes\n",
    "\n",
    "@dispatch(ProductLayer, np.ndarray, ll_cache=dict, sampling_context=SamplingContext)\n",
    "def sample(module: ProductLayer, data: np.ndarray, ll_cache: dict={}, sampling_context=None) -> np.ndarray:\n",
    "    \"\"\"Samples leaf nodes accoding to sampling context.\"\"\"\n",
    "    \n",
    "    if sampling_context:\n",
    "        if not sampling_context.is_valid(data, module):\n",
    "            # invalid sampling context\n",
    "            raise ValueError(f\"Specified sampling context is invalid for specified data array and module.\")\n",
    "    else:\n",
    "        raise ValueError(\"No sampling context specified. It is unclear which output to sample from.\")\n",
    "\n",
    "\n",
    "    for node_ids in np.unique(sampling_context.output_ids, axis=0):\n",
    "        if(len(node_ids) != 1):\n",
    "            raise ValueError(\"Too many output ids specified for outputs over same scope.\")\n",
    "\n",
    "        node_id = node_ids[0]\n",
    "        node_instance_ids = np.where(sampling_context.output_ids == node_ids)[0]\n",
    "        sample(module.nodes[node_id], data, ll_cache=ll_cache, sampling_context=SamplingContext(node_instance_ids, [[] for i in node_instance_ids]))\n",
    "        \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood: [[0.07119906 0.07119906 0.07119906 0.07119906]]\n",
      "Scope: [[Scope({0, 1}) Scope({0, 1}) Scope({0, 1}) Scope({0, 1})]]\n"
     ]
    }
   ],
   "source": [
    "leaf_layer_1 = LeafLayer(Scope(SPN(), [0]), 2)\n",
    "leaf_layer_2 = LeafLayer(Scope(SPN(), [1]), 2)\n",
    "product_layer = ProductLayer([leaf_layer_1, leaf_layer_2])\n",
    "\n",
    "data = np.random.randn(1,2)\n",
    "print(f\"Likelihood: {likelihood(product_layer, data)}\")\n",
    "\n",
    "print(f\"Scope: {scope(product_layer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood: [[0.07730176]]\n",
      "Scope: [[Scope({0, 1})]]\n"
     ]
    }
   ],
   "source": [
    "l1 = LeafLayer(Scope(SPN(), [0]), n_out=3)\n",
    "s1 = SumLayer(3, [l1])\n",
    "\n",
    "l2 = LeafLayer(Scope(SPN(), [1]), n_out=3)\n",
    "s2 = SumLayer(3, [l2])\n",
    "\n",
    "p = ProductLayer([s1,s2])\n",
    "s = SumNode([p])\n",
    "\n",
    "data = np.random.randn(1,2)\n",
    "print(f\"Likelihood: {likelihood(s, data)}\")\n",
    "\n",
    "print(f\"Scope: {scope(s)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.45662226, 0.02762445]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood: [[0.05911936]]\n",
      "Likelihood2: [[0.05911936]]\n",
      "Scope: [[Scope({0, 1})]]\n",
      "Scope2: [[Scope({0, 1})]]\n",
      "Sample1: [-0.00088852 -0.00579437]\n",
      "Sample2: [0.00094516 0.00203483]\n"
     ]
    }
   ],
   "source": [
    "l1 = LeafLayer(Scope(SPN(), [0]), n_out=2)\n",
    "s1 = SumLayer(2, [l1])\n",
    "\n",
    "l2 = LeafLayer(Scope(SPN(), [1]), n_out=2)\n",
    "s2 = SumLayer(2, [l2])\n",
    "\n",
    "p = ProductLayer([s1,s2])\n",
    "s = SumNode([p])\n",
    "\n",
    "lf1 = LeafNode(Scope(SPN(), [0]))\n",
    "lf2 = LeafNode(Scope(SPN(), [0]))\n",
    "lf3 = LeafNode(Scope(SPN(), [1]))\n",
    "lf4 = LeafNode(Scope(SPN(), [1]))\n",
    "\n",
    "s11 = SumNode([lf1, lf2])\n",
    "s12 = SumNode([lf1, lf2])\n",
    "s21 = SumNode([lf3, lf4])\n",
    "s22 = SumNode([lf3, lf4])\n",
    "\n",
    "p1 = ProductNode([s11, s21])\n",
    "p2 = ProductNode([s11, s22])\n",
    "p3 = ProductNode([s12, s21])\n",
    "p4 = ProductNode([s12, s22])\n",
    "\n",
    "s1 = SumNode([p1, p2, p3, p4])\n",
    "\n",
    "data = np.random.randn(1,2)\n",
    "print(f\"Likelihood: {likelihood(s, data)}\")\n",
    "print(f\"Likelihood2: {likelihood(s1, data)}\")\n",
    "\n",
    "\n",
    "print(f\"Scope: {scope(s)}\")\n",
    "print(f\"Scope2: {scope(s1)}\")\n",
    "print(f\"Sample1: {np.mean(sample(s, 100000), axis=0)}\")\n",
    "print(f\"Sample2: {np.mean(sample(s1, 100000), axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks\n",
    "\n",
    "We could now build even more complex modules or networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleNetwork(NestingModule):\n",
    "    \"\"\"Example network using layers and nodes.\n",
    "    \n",
    "    Args:\n",
    "        children: list of child modules.\n",
    "    \"\"\"\n",
    "    def __init__(self, children: List[Module]) -> None:\n",
    "        super(ExampleNetwork, self).__init__(children)\n",
    "        \n",
    "        n_ins = [child.n_out for child in self.children]\n",
    "        placeholders = []\n",
    "        \n",
    "        total_ids = range(sum(n_ins))\n",
    "\n",
    "        for n in n_ins:\n",
    "            placeholders.append(self.create_placeholder(total_ids[:n]))\n",
    "            total_ids = total_ids[n:]\n",
    "        \n",
    "        # create product layer on top\n",
    "        self.product_layer = ProductLayer(children=placeholders)\n",
    "        \n",
    "        # sum over all product layers\n",
    "        self.sum_node = SumNode(children=[self.product_layer])\n",
    "    \n",
    "    @property\n",
    "    def n_out(self) -> int:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we can dispatch to the sum node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dispatch(ExampleNetwork, cache=dict)\n",
    "@memoize\n",
    "def scope(module: ExampleNetwork, cache: dict={}) -> ScopeArray:\n",
    "    # compute input scopes\n",
    "    input_scopes = np.concatenate([scope(child, cache=cache) for child in module.children], axis=1)\n",
    "    \n",
    "    # set placeholders\n",
    "    module.set_placeholders(cache, input_scopes)\n",
    "    \n",
    "    return scope(module.sum_node, cache=cache)\n",
    "\n",
    "@dispatch(ExampleNetwork, np.ndarray, cache=dict)\n",
    "@memoize\n",
    "def likelihood(module: ExampleNetwork, data: np.ndarray, cache: dict={}) -> np.ndarray:\n",
    "    # compute input likelihoods\n",
    "    input_likelihoods = np.concatenate([likelihood(child, data, cache=cache) for child in module.children], axis=1)\n",
    "    \n",
    "    # set placeholders\n",
    "    module.set_placeholders(cache, input_likelihoods)\n",
    "    \n",
    "    return likelihood(module.sum_node, data, cache=cache)\n",
    "\n",
    "@dispatch(ExampleNetwork, np.ndarray, ll_cache=dict, sampling_context=SamplingContext)\n",
    "def sample(module: ExampleNetwork, data: np.ndarray, ll_cache: dict={}, sampling_context=None) -> np.ndarray:\n",
    "    likelihood(module, data, cache=ll_cache)\n",
    "    return sample(module.sum_node, data, ll_cache=ll_cache, sampling_context=sampling_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "placeholder test\n",
      "range(0, 2)\n",
      "<__main__.ExampleNetwork object at 0x7fb5e917e310>\n",
      "{0: (0, 0), 1: (0, 1), 2: (1, 0), 3: (1, 1), 4: (2, 0), 5: (2, 1)}\n",
      "input test\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "placeholder test\n",
      "range(2, 4)\n",
      "<__main__.ExampleNetwork object at 0x7fb5e917e310>\n",
      "{0: (0, 0), 1: (0, 1), 2: (1, 0), 3: (1, 1), 4: (2, 0), 5: (2, 1)}\n",
      "input test\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "placeholder test\n",
      "range(4, 6)\n",
      "<__main__.ExampleNetwork object at 0x7fb5e917e310>\n",
      "{0: (0, 0), 1: (0, 1), 2: (1, 0), 3: (1, 1), 4: (2, 0), 5: (2, 1)}\n",
      "input test\n",
      "0\n",
      "4\n",
      "1\n",
      "5\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "[2, 3, 4, 5]\n",
      "[4, 5]\n",
      "[]\n",
      "init prod layer\n",
      "[2, 2, 2]\n",
      "[]\n",
      "[[0, 1], [2, 3], [4, 5]]\n",
      "[(0, 2, 4), (0, 2, 5), (0, 3, 4), (0, 3, 5), (1, 2, 4), (1, 2, 5), (1, 3, 4), (1, 3, 5)]\n",
      "prod_ids test\n",
      "[0, 2, 4]\n",
      "placeholder test\n",
      "[0, 2, 4]\n",
      "<__main__.ProductLayer object at 0x7fb5e9173090>\n",
      "{0: (0, 0), 1: (0, 1), 2: (1, 0), 3: (1, 1), 4: (2, 0), 5: (2, 1)}\n",
      "input test\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "4\n",
      "prod_ids test\n",
      "[0, 2, 5]\n",
      "placeholder test\n",
      "[0, 2, 5]\n",
      "<__main__.ProductLayer object at 0x7fb5e9173090>\n",
      "{0: (0, 0), 1: (0, 1), 2: (1, 0), 3: (1, 1), 4: (2, 0), 5: (2, 1)}\n",
      "input test\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "5\n",
      "prod_ids test\n",
      "[0, 3, 4]\n",
      "placeholder test\n",
      "[0, 3, 4]\n",
      "<__main__.ProductLayer object at 0x7fb5e9173090>\n",
      "{0: (0, 0), 1: (0, 1), 2: (1, 0), 3: (1, 1), 4: (2, 0), 5: (2, 1)}\n",
      "input test\n",
      "0\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "4\n",
      "prod_ids test\n",
      "[0, 3, 5]\n",
      "placeholder test\n",
      "[0, 3, 5]\n",
      "<__main__.ProductLayer object at 0x7fb5e9173090>\n",
      "{0: (0, 0), 1: (0, 1), 2: (1, 0), 3: (1, 1), 4: (2, 0), 5: (2, 1)}\n",
      "input test\n",
      "0\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "5\n",
      "prod_ids test\n",
      "[1, 2, 4]\n",
      "placeholder test\n",
      "[1, 2, 4]\n",
      "<__main__.ProductLayer object at 0x7fb5e9173090>\n",
      "{0: (0, 0), 1: (0, 1), 2: (1, 0), 3: (1, 1), 4: (2, 0), 5: (2, 1)}\n",
      "input test\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "4\n",
      "prod_ids test\n",
      "[1, 2, 5]\n",
      "placeholder test\n",
      "[1, 2, 5]\n",
      "<__main__.ProductLayer object at 0x7fb5e9173090>\n",
      "{0: (0, 0), 1: (0, 1), 2: (1, 0), 3: (1, 1), 4: (2, 0), 5: (2, 1)}\n",
      "input test\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "5\n",
      "prod_ids test\n",
      "[1, 3, 4]\n",
      "placeholder test\n",
      "[1, 3, 4]\n",
      "<__main__.ProductLayer object at 0x7fb5e9173090>\n",
      "{0: (0, 0), 1: (0, 1), 2: (1, 0), 3: (1, 1), 4: (2, 0), 5: (2, 1)}\n",
      "input test\n",
      "0\n",
      "1\n",
      "1\n",
      "3\n",
      "2\n",
      "4\n",
      "prod_ids test\n",
      "[1, 3, 5]\n",
      "placeholder test\n",
      "[1, 3, 5]\n",
      "<__main__.ProductLayer object at 0x7fb5e9173090>\n",
      "{0: (0, 0), 1: (0, 1), 2: (1, 0), 3: (1, 1), 4: (2, 0), 5: (2, 1)}\n",
      "input test\n",
      "0\n",
      "1\n",
      "1\n",
      "3\n",
      "2\n",
      "5\n",
      "Likelihoods: [[0.03581267]\n",
      " [0.0298803 ]]\n",
      "Scope: [[Scope({0, 1, 2})]]\n"
     ]
    }
   ],
   "source": [
    "leaf_layers = [LeafLayer(Scope(SPN(), [i]), 2) for i in range(3)]\n",
    "net = ExampleNetwork(children=leaf_layers)\n",
    "\n",
    "data = np.random.rand(2,3)\n",
    "\n",
    "print(f\"Likelihoods: {likelihood(net, data)}\")\n",
    "print(f\"Scope: {scope(net)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs/Open Questions\n",
    "\n",
    "* There is still some bug in `ProductLayer` that returns `NaN`s when sampling (not a concept issue, but should be fixed for better demonstration)\n",
    "* Examples for optimized layers without explicit nodes.\n",
    "* Structural marginalization\n",
    "* Must make sure that likelihoods are computed in the very beginning and don't change (i.e. partly sampled data should not affect the likelihoods of the rest of the sampling routine)\n",
    "* Creating default sampling contexts, computing input (scopes, likelihoods) etc. could be handled via specific decorator for sampling, scope and likelihood computation\n",
    "* Sampling multiple non-disjoint outputs at the same time (e.g. different replicas in RAT-SPNs)\n",
    "* Case distinction for scopes (e.g. merging based on sum-node different than product node). So far, modules themselves would have to check scopes manually.\n",
    "* Scope variable order: e.g. for `LeafNode` the order of the scope might be important, but is a `set` here. For example, a multivariate Gaussian could be specified via scope `[0,1]` or `[1,0]`, depending on the desired order. Since the data is selected via the `set`, the order might be different from the specified `mean`,`std` values, which might not be what the user expects.\n",
    "* Incorporate conditional modules (C-SPNs) and therefore evidence variables in scopes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Modules\n",
    "\n",
    "One might want to create conditional modules, e.g. modules where parameters are conditioned on some inputs and set accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalModule(Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
